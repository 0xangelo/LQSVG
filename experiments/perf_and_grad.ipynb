{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "quiet-success",
   "metadata": {},
   "source": [
    "# Ground-truth performance and gradient norms\n",
    "\n",
    "This notebook inspects the initialization schemes for LQGs and time-varying linear policies described in section 5.1 of the [paper](https://www.overleaf.com/read/cmbgmxxpxqzr).\n",
    "\n",
    "### Checklist\n",
    "\n",
    "- [x] Fix `n_state`, `n_ctrl`, `horizon`\n",
    "- [x] Sample random LQGs\n",
    "- [x] Sample random policies\n",
    "- [x] Evaluate the expected return\n",
    "- [x] Evaluate the value gradient norm\n",
    "- [x] Search numpy, scipy for methods for visualizing the distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-intelligence",
   "metadata": {},
   "source": [
    "---\n",
    "## Linear Quadratic Gaussian (LQG) Problems\n",
    "In what follows we consider MDPs with:\n",
    "1. continuous state space $\\mathbf{s} \\in \\mathcal{S} = \\mathbb{R}^n$\n",
    "2. continuous action space $\\mathbf{a} \\in \\mathcal{A} = \\mathbb{R}^d$\n",
    "3. finite time horizon $N \\in \\mathbb{N}$ and timesteps $t \\in \\mathcal{T} = \\{0, \\dots, N - 1\\}$\n",
    "4. time-varying linear Gaussian dynamics \n",
    "    $$\n",
    "    \\mathbf{s}_{t+1} \\sim p(\\cdot| \\mathbf{s}_t, \\mathbf{a}_t) = \\mathcal{N}\\left( \\cdot ~\\middle|~ \\mathbf{F}_t \\begin{bmatrix}\\mathbf{s}_t \\\\ \\mathbf{a}_t\\end{bmatrix} + \\mathbf{f}_t, \\mathbf{\\Sigma}_{t} \\right)\n",
    "    $$\n",
    "5. time-varying quadratic costs \n",
    "    $$\n",
    "    r_{t+1} = R(\\mathbf{s}_t, \\mathbf{a}_t) = - \\tfrac{1}{2} \\begin{bmatrix}\\mathbf{s}_t \\\\ \\mathbf{a}_t\\end{bmatrix}^\\intercal \\mathbf{C}_t \\begin{bmatrix}\\mathbf{s}_t \\\\ \\mathbf{a}_t\\end{bmatrix} - \\mathbf{c}_t^\\intercal \\begin{bmatrix}\\mathbf{s}_t \\\\ \\mathbf{a}_t\\end{bmatrix}\n",
    "    $$\n",
    "6. Gaussian-distributed initial state \n",
    "    $$\n",
    "    \\mathbf{s}_0 \\sim \\rho = \\mathcal{N}(\\mathbf{\\mu}_\\rho, \\mathbf{\\Sigma}_\\rho)\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-transcription",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "decimal-broadcast",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import annotations\n",
    "\n",
    "import textwrap\n",
    "import warnings\n",
    "from functools import partial\n",
    "from typing import Callable, Optional\n",
    "\n",
    "import lqsvg.torch.named as nt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from lqsvg.envs import lqr\n",
    "from lqsvg.envs.lqr.gym import LQGGenerator\n",
    "from lqsvg.experiment.models import ExpectedValue, PolicyLoss\n",
    "from lqsvg.experiment.utils import linear_feedback_distance, linear_feedback_norm\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.stats import ortho_group\n",
    "from torch import Tensor\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-airfare",
   "metadata": {},
   "source": [
    "---\n",
    "## Task selection protocol\n",
    "\n",
    "$$\n",
    "    \\{ \\mathbf{F}_t, \\mathbf{f}_t, \\mathbf{\\Sigma}_t, \\mathbf{C}_t, \\mathbf{c}_t  \\}_{t\\in\\mathcal{T}} \\sim \\text{LQGDist}(\\texttt{n_state, n_ctrl, horizon}) \\\\\n",
    "    \\mathbf{\\mu}_\\rho = \\mathbf{0}, \\quad \\mathbf{\\Sigma}_{\\rho} = \\mathbf{I} \\\\\n",
    "    \\mathcal{M} = \\{ \\mathbf{F}_t, \\mathbf{f}_t, \\mathbf{\\Sigma}_t, \\mathbf{C}_t, \\mathbf{c}_t  \\}_{t\\in\\mathcal{T}} \\cup \\{ \\mathbf{\\mu}_\\rho, \\mathbf{\\Sigma}_\\rho \\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "international-detector",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_lqg(\n",
    "    n_state: int, n_ctrl: int, horizon: int, n_batch: Optional[int] = None\n",
    ") -> tuple[lqr.LinSDynamics, lqr.QuadCost, lqr.GaussInit]:\n",
    "    generator = LQGGenerator(\n",
    "        n_state,\n",
    "        n_ctrl,\n",
    "        horizon,\n",
    "        stationary=True,\n",
    "        Fs_eigval_range=(0.0, 2.0),\n",
    "        transition_bias=False,\n",
    "        rand_trans_cov=True,\n",
    "        rand_init_cov=False,\n",
    "        cost_linear=False,\n",
    "        cost_cross=False,\n",
    "    )\n",
    "    dynamics, cost, init = generator(n_batch)\n",
    "\n",
    "    F_s, F_a = nt.split(dynamics.F, [n_state, n_ctrl], dim=\"C\")\n",
    "    F_a = F_a / (torch.linalg.norm(nt.unnamed(F_a), dim=(-2, -1), keepdim=True) + 1e-8)\n",
    "    dynamics = lqr.LinSDynamics(\n",
    "        F=torch.cat([F_s, F_a], dim=\"C\"), f=dynamics.f, W=dynamics.W\n",
    "    )\n",
    "\n",
    "    return dynamics, cost, init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "drawn-minister",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_lqg(dynamics, cost, init):\n",
    "    msg = f\"\"\"\\\n",
    "        dynamics:\n",
    "            F: {dynamics.F.shape}; {dynamics.F.names}; min={dynamics.F.min()}; max={dynamics.F.max()}\n",
    "            f: {dynamics.f.shape}; {dynamics.f.names}; min={dynamics.f.min()}; max={dynamics.f.max()}\n",
    "            Sig: {dynamics.W.shape}; {dynamics.W.names}\n",
    "        cost:\n",
    "            C: {cost.C.shape}; {cost.C.names}            \n",
    "            c: {cost.c.shape}; {cost.c.names}\n",
    "        init:\n",
    "            mu: {init.mu.shape}; {init.mu.names}            \n",
    "            sig: {init.sig.shape}; {init.sig.names}\\\n",
    "        \"\"\"\n",
    "    print(textwrap.dedent(msg))\n",
    "    if \"B\" not in cost.C.names:\n",
    "        print(\"C_0:\")\n",
    "        print(textwrap.indent(str(cost.C.select(\"H\", 0).numpy()), \" \" * 4))\n",
    "\n",
    "\n",
    "def test_sample_lqg(n_batch=None):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", message=\".*Named tensors.*\", module=\"torch\")\n",
    "        dynamics, cost, init = sample_lqg(\n",
    "            n_state=2, n_ctrl=2, horizon=100, n_batch=n_batch\n",
    "        )\n",
    "    assert isinstance(dynamics, lqr.LinSDynamics)\n",
    "    assert isinstance(cost, lqr.QuadCost)\n",
    "    assert isinstance(init, lqr.GaussInit)\n",
    "    print_lqg(dynamics, cost, init)\n",
    "\n",
    "\n",
    "def test_batch_lqgs():\n",
    "    test_sample_lqg(n_batch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "guided-freedom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dynamics:\n",
      "    F: torch.Size([100, 2, 4]); ('H', 'R', 'C'); min=-1.213716745376587; max=0.16405349969863892\n",
      "    f: torch.Size([100, 2]); ('H', 'R'); min=0.0; max=0.0\n",
      "    Sig: torch.Size([100, 2, 2]); ('H', 'R', 'C')\n",
      "cost:\n",
      "    C: torch.Size([100, 4, 4]); ('H', 'R', 'C')            \n",
      "    c: torch.Size([100, 4]); ('H', 'R')\n",
      "init:\n",
      "    mu: torch.Size([2]); ('R',)            \n",
      "    sig: torch.Size([2, 2]); ('R', 'C')        \n",
      "C_0:\n",
      "    [[ 2.274646    2.0066602   0.          0.        ]\n",
      "     [ 2.0066602   2.6473014   0.          0.        ]\n",
      "     [ 0.          0.          0.778606   -0.0830318 ]\n",
      "     [ 0.          0.         -0.0830318   0.76030445]]\n"
     ]
    }
   ],
   "source": [
    "test_sample_lqg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "worthy-complex",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dynamics:\n",
      "    F: torch.Size([100, 10, 2, 4]); ('H', 'B', 'R', 'C'); min=-1.8809062242507935; max=0.8916661143302917\n",
      "    f: torch.Size([100, 10, 2]); ('H', 'B', 'R'); min=0.0; max=0.0\n",
      "    Sig: torch.Size([100, 10, 2, 2]); ('H', 'B', 'R', 'C')\n",
      "cost:\n",
      "    C: torch.Size([100, 10, 4, 4]); ('H', 'B', 'R', 'C')            \n",
      "    c: torch.Size([100, 10, 4]); ('H', 'B', 'R')\n",
      "init:\n",
      "    mu: torch.Size([10, 2]); ('B', 'R')            \n",
      "    sig: torch.Size([10, 2, 2]); ('B', 'R', 'C')        \n"
     ]
    }
   ],
   "source": [
    "test_batch_lqgs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-education",
   "metadata": {},
   "source": [
    "---\n",
    "## Policy selection protocol\n",
    "\n",
    "$$\n",
    "    \\mu_\\theta(\\mathbf{s}) = \\mathbf{a} \\\\\n",
    "    \\mathcal{M} \\sim \\text{LQGDist}(\\texttt{n_state, n_ctrl, horizon}) \\\\\n",
    "    \\theta = \\{ \\mathbf{K}_t, \\mathbf{k}_t \\}_{t\\in\\mathcal{T}} \\sim \\text{PiDist}(\\mathcal{M})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "retired-reserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solution(\n",
    "    dynamics: lqr.LinSDynamics, cost: lqr.QuadCost\n",
    ") -> tuple[lqr.Linear, lqr.Quadratic, lqr.Quadratic]:\n",
    "    n_state, n_ctrl, horizon = lqr.dims_from_dynamics(dynamics)\n",
    "    solver = lqr.NamedLQGControl(n_state, n_ctrl, horizon)\n",
    "    pistar, qstar, vstar = solver(dynamics, cost)\n",
    "    return pistar, qstar, vstar\n",
    "\n",
    "\n",
    "def optimal_policy(dynamics: lqr.LinSDynamics, cost: lqr.QuadCost) -> lqr.Linear:\n",
    "    pistar, _, _ = solution(dynamics, cost)\n",
    "    return pistar\n",
    "\n",
    "\n",
    "def perturb_policy(policy: lqr.Linear) -> lqr.Linear:\n",
    "    n_state, n_ctrl, _ = lqr.dims_from_policy(policy)\n",
    "    K, k = (g + 0.5 * torch.randn_like(g) / (n_state + np.sqrt(n_ctrl)) for g in policy)\n",
    "    #     scale = 1 / (n_state + np.sqrt(n_ctrl))\n",
    "    #     K, k = (g + 2 * torch.rand_like(g) * scale - scale for g in policy)\n",
    "    return (K, k)\n",
    "\n",
    "\n",
    "def sample_policy(dynamics: lqr.LinSDynamics, cost: lqr.QuadCost) -> lqr.Linear:\n",
    "    n_state, n_ctrl, horizon = lqr.dims_from_dynamics(dynamics)\n",
    "    K = torch.empty((horizon,) + dynamics.F.shape[1:-2] + (n_ctrl, n_state))\n",
    "    k = torch.empty((horizon,) + dynamics.F.shape[1:-2] + (n_ctrl,))\n",
    "    nn.init.xavier_uniform_(K)\n",
    "    nn.init.constant_(k, 0)\n",
    "    random = (nt.horizon(nt.matrix(K)), nt.horizon(nt.vector(k)))\n",
    "\n",
    "    #     pistar = optimal_policy(dynamics, cost)\n",
    "    #     random = perturb_policy(pistar)\n",
    "    return random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "opposite-tucson",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_policy(policy: lqr.Linear):\n",
    "    K, k = policy\n",
    "    print(\n",
    "        f\"\"\"\n",
    "        K: {K.shape}; {K.names}\n",
    "        k: {k.shape}; {k.names}\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "def test_optimal_policy():\n",
    "    dynamics, cost, _ = sample_lqg(n_state=2, n_ctrl=2, horizon=100, n_batch=10)\n",
    "    print_policy(optimal_policy(dynamics, cost))\n",
    "\n",
    "\n",
    "def test_sample_policy():\n",
    "    dynamics, cost, _ = sample_lqg(n_state=2, n_ctrl=2, horizon=100, n_batch=10)\n",
    "    print_policy(sample_policy(dynamics, cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "handled-stream",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        K: torch.Size([100, 10, 2, 2]); ('H', None, 'R', 'C')\n",
      "        k: torch.Size([100, 10, 2]); ('H', None, 'R')\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "test_optimal_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "competitive-algebra",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        K: torch.Size([100, 10, 2, 2]); ('H', None, 'R', 'C')\n",
      "        k: torch.Size([100, 10, 2]); ('H', None, 'R')\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "test_sample_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "hazardous-effort",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_distance(policy_a: lqr.Linear, policy_b: lqr.Linear) -> Tensor:\n",
    "    return linear_feedback_distance(policy_a, policy_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "stopped-swedish",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_policy_distance():\n",
    "    dynamics, cost, _ = sample_lqg(n_state=10, n_ctrl=10, horizon=100, n_batch=10)\n",
    "    optimal = optimal_policy(dynamics, cost)\n",
    "    random = perturb_policy(optimal)\n",
    "    distance = policy_distance(optimal, random)\n",
    "    print(\n",
    "        f\"\"\"\\\n",
    "    Policy distances: {distance}, {distance.shape}\\\n",
    "    \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cardiovascular-dining",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Policy distances: tensor([3.9913, 3.9705, 4.0332, 3.9826, 3.9669, 3.9685, 3.9635, 3.9673, 3.9614,\n",
      "        4.0183]), torch.Size([10])    \n"
     ]
    }
   ],
   "source": [
    "test_policy_distance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-economics",
   "metadata": {},
   "source": [
    "---\n",
    "## Ground-truth policy performance\n",
    "\n",
    "<center><b>Sampling</b></center>\n",
    "$$\n",
    "    \\mathcal{M} \\sim \\text{LQGDist}(\\texttt{n_state, n_ctrl, horizon}) \\\\\n",
    "    \\theta = \\{ \\mathbf{K}_t, \\mathbf{k}_t \\}_{t\\in\\mathcal{T}} \\sim \\text{PiDist}(\\mathcal{M})\n",
    "$$\n",
    "<br>\n",
    "\n",
    "<center><b>Prediction</b></center>\n",
    "<!-- LQG prediction can be seen as a function mapping policy parameters (with the dynamics kept constant) to value function coefficients: -->\n",
    "$$\n",
    "\\left( \\mathbf{Q}, \\mathbf{q}, q, \\mathbf{V}, \\mathbf{v}, v \\right) = \\text{LQGPrediction}(\\theta) \\\\\n",
    "V^\\mu(\\mathbf{s}, t) = -\\tfrac12 \\mathbf{s}^\\intercal \\mathbf{V}_t(\\theta) \\mathbf{s} - \\mathbf{v}_t(\\theta)^\\intercal \\mathbf{s} - v_t(\\theta)\n",
    "$$\n",
    "<br>\n",
    "\n",
    "<center><b>Performance</b></center>\n",
    "<!-- We can then express policy performance as a direct function of policy parameters: -->\n",
    "$$\n",
    "\\begin{align}\n",
    "J(\\theta) \n",
    "    &= -\\mathbb{E}_{\\mathbf{s}\\sim\\rho} \\left[ \\tfrac12 \\mathbf{s}^\\intercal \\mathbf{V}_0(\\theta) \\mathbf{s} + \\mathbf{v}_0(\\theta)^\\intercal \\mathbf{s} + v_0(\\theta) \\right] \\\\\n",
    "    &= -\\text{Tr}(\\mathbf{V}_0(\\theta)\\mathbf{\\Sigma}_0) - \\mathbf{\\mu}_0^\\intercal \\mathbf{V}_0(\\theta) \\mathbf{\\mu}_0 - \\mathbf{v}_0(\\theta)^\\intercal \\mathbf{\\mu}_0 - v_0(\\theta)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "consolidated-girlfriend",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_loss(\n",
    "    policy: lqr.Linear,\n",
    "    dynamics: lqr.LinSDynamics,\n",
    "    cost: lqr.QuadCost,\n",
    "    init: lqr.GaussInit,\n",
    ") -> Tensor:\n",
    "    n_state, n_ctrl, horizon = lqr.dims_from_dynamics(dynamics)\n",
    "    loss_fn = PolicyLoss(n_state, n_ctrl, horizon)\n",
    "    loss = loss_fn(policy, dynamics, cost, init)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def policy_performance(\n",
    "    policy: lqr.Linear,\n",
    "    dynamics: lqr.LinSDynamics,\n",
    "    cost: lqr.QuadCost,\n",
    "    init: lqr.GaussInit,\n",
    ") -> Tensor:\n",
    "    return -policy_loss(policy, dynamics, cost, init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "israeli-residence",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_policy_loss():\n",
    "    dynamics, cost, init = sample_lqg(n_state=2, n_ctrl=2, horizon=20, n_batch=10)\n",
    "    policy = sample_policy(dynamics, cost)\n",
    "    loss = policy_loss(policy, dynamics, cost, init)\n",
    "    print(\n",
    "        f\"\"\"\\\n",
    "    Loss: {loss}, ({loss.dtype}), ({loss.shape}); \n",
    "    ExpectedReturn: {-loss}, ({(-loss).dtype}), ({loss.shape})\\\n",
    "    \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "configured-blade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loss: tensor([1.8992e+07, 3.2380e+03, 1.4692e+06, 5.8283e+08, 2.0671e+06, 1.2380e+04,\n",
      "        1.2105e+08, 2.7014e+03, 1.4148e+11, 1.2227e+02], names=('B',)), (torch.float32), (torch.Size([10])); \n",
      "    ExpectedReturn: tensor([-1.8992e+07, -3.2380e+03, -1.4692e+06, -5.8283e+08, -2.0671e+06,\n",
      "        -1.2380e+04, -1.2105e+08, -2.7014e+03, -1.4148e+11, -1.2227e+02],\n",
      "       names=('B',)), (torch.float32), (torch.Size([10]))    \n"
     ]
    }
   ],
   "source": [
    "test_policy_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-techno",
   "metadata": {},
   "source": [
    "---\n",
    "## Ground-truth value gradient norm\n",
    "\n",
    "1. **Sampling**$\\rightarrow \\mathcal{M}, \\theta$; \n",
    "2. **Prediction**$\\rightarrow \\left( \\mathbf{Q}, \\mathbf{q}, q, \\mathbf{V}, \\mathbf{v}, v \\right)$; \n",
    "3. **Performance**$\\rightarrow J(\\theta)$;\n",
    "4. **SVG norm**$\\rightarrow \\| \\nabla J(\\theta) \\|$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-consortium",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_batched_svg_norm(exp_ret: Tensor, policy: lqr.Linear) -> Tensor:\n",
    "    for x in policy:\n",
    "        x.grad = None\n",
    "    exp_ret.sum().backward()\n",
    "\n",
    "    K_grad, k_grad = (x.grad.detach() for x in policy)\n",
    "    return linear_feedback_norm((K_grad, k_grad))\n",
    "\n",
    "\n",
    "def policy_svg_norm(\n",
    "    policy: lqr.Linear,\n",
    "    dynamics: lqr.LinSDynamics,\n",
    "    cost: lqr.QuadCost,\n",
    "    init: lqr.GaussInit,\n",
    ") -> tuple[Tensor, Tensor]:\n",
    "    eret = policy_performance(policy, dynamics, cost, init)\n",
    "    svg_norm = compute_batched_svg_norm(eret, policy)\n",
    "    return eret, svg_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-greenhouse",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_random_policy_svg():\n",
    "    dynamics, cost, init = sample_lqg(n_state=2, n_ctrl=2, horizon=100, n_batch=10)\n",
    "    policy = tuple(x.requires_grad_(True) for x in sample_policy(dynamics, cost))\n",
    "    eret, svg_norm = policy_svg_norm(policy, dynamics, cost, init)\n",
    "    print(f\"Expected return: {eret}, ({eret.dtype}, {eret.shape})\")\n",
    "    print(f\"SVG norm: {svg_norm} ({svg_norm.dtype}, {svg_norm.shape})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-leather",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_random_policy_svg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-lucas",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_optimal_policy_svg():\n",
    "    dynamics, cost, init = sample_lqg(n_state=2, n_ctrl=2, horizon=100, n_batch=10)\n",
    "    pistar, _, vstar = solution(dynamics, cost)\n",
    "    loss_fn = ExpectedValue()\n",
    "    eret = -loss_fn(init, tuple(x.select(\"H\", 0) for x in vstar))\n",
    "    print(f\"Optimal expected return: {eret} ({eret.dtype})\")\n",
    "\n",
    "    policy = tuple(x.requires_grad_(True) for x in pistar)\n",
    "    eret = -policy_loss(policy, dynamics, cost, init)\n",
    "    print(f\"Expected return from Prediction algorithm: {eret} ({eret.dtype})\")\n",
    "    svg_norm = compute_batched_svg_norm(eret, policy)\n",
    "    print(f\"Optimal policy SVG norm: {svg_norm} ({svg_norm.dtype}, {svg_norm.shape})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-aggregate",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_optimal_policy_svg()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-french",
   "metadata": {},
   "source": [
    "---\n",
    "## Beyond averages\n",
    "\n",
    "Next, we evaluate the distribution of optimal/random policy performances for certain LQG problem sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respected-arlington",
   "metadata": {},
   "source": [
    "### Calculating the expected return (performance) & SVG norm for a batch of LQGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-honduras",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_return(init: lqr.GaussInit, vval: lqr.Quadratic) -> Tensor:\n",
    "    expected_value = ExpectedValue()\n",
    "    expected_cost = expected_value(init, tuple(x.select(\"H\", 0) for x in vval))\n",
    "    return -expected_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-powder",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def performance_samples(\n",
    "    dims: tuple[int, int, int],\n",
    "    optimal: bool = False,\n",
    "    samples: int = 100,\n",
    "    clip_ctrl_dim: bool = False,\n",
    ") -> np.ndarray:\n",
    "    n_state, n_ctrl, horizon = dims\n",
    "    if clip_ctrl_dim and n_ctrl < n_state:\n",
    "        return np.full((samples,), fill_value=np.nan)\n",
    "\n",
    "    dynamics, cost, init = sample_lqg(n_state, n_ctrl, horizon, n_batch=samples)\n",
    "\n",
    "    if optimal:\n",
    "        pistar, qstar, vstar = solution(dynamics, cost)\n",
    "        eret = expected_return(init, vstar)\n",
    "    else:\n",
    "        policy = sample_policy(dynamics, cost)\n",
    "        eret = policy_performance(policy, dynamics, cost, init)\n",
    "\n",
    "    return eret.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-undergraduate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svg_norm_samples(\n",
    "    dims: tuple[int, int, int], samples: int = 100, clip_ctrl_dim: bool = False\n",
    ") -> np.ndarray:\n",
    "    n_state, n_ctrl, horizon = dims\n",
    "    if clip_ctrl_dim and n_ctrl < n_state:\n",
    "        return np.full((samples,), fill_value=np.nan)\n",
    "\n",
    "    dynamics, cost, init = sample_lqg(n_state, n_ctrl, horizon, n_batch=samples)\n",
    "    policy = tuple(g.requires_grad_(True) for g in sample_policy(dynamics, cost))\n",
    "    _, svg_norm = policy_svg_norm(policy, dynamics, cost, init)\n",
    "    return svg_norm.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-hello",
   "metadata": {},
   "source": [
    "### Plotting the performance distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-fabric",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_optimal_vs_random_performances_hist(\n",
    "    dims: tuple[int, int, int], samples: int = 1000\n",
    "):\n",
    "    optimal = pd.DataFrame(\n",
    "        {\"performance\": performance_samples(dims, optimal=True, samples=samples)}\n",
    "    )\n",
    "    random = pd.DataFrame(\n",
    "        {\"performance\": performance_samples(dims, optimal=False, samples=samples)}\n",
    "    )\n",
    "\n",
    "    optimal[\"optimal\"] = True\n",
    "    random[\"optimal\"] = False\n",
    "    data = pd.concat([optimal, random])\n",
    "\n",
    "    sns.histplot(\n",
    "        data=data,\n",
    "        x=\"performance\",\n",
    "        hue=\"optimal\",\n",
    "        hue_order=[True, False],\n",
    "        stat=\"density\",\n",
    "        #         bins=samples // 10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-shakespeare",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimal_vs_random_performances_hist((2, 2, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-exemption",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimal_vs_random_performances_hist((2, 2, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-usage",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimal_vs_random_performances_hist((4, 4, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-tulsa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimal_vs_random_performances_hist((10, 10, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-texas",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimal_vs_random_performances_hist((10, 10, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-twenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimal_vs_random_performances_hist((20, 20, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judicial-tomorrow",
   "metadata": {},
   "source": [
    "### Plotting the SVG norm distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-utility",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svg_norm_hist(dims: tuple[int, int, int], samples: int = 1000):\n",
    "    data = pd.DataFrame({\"svg norm\": svg_norm_samples(dims, samples=samples)})\n",
    "    sns.histplot(data=data, x=\"svg norm\", stat=\"density\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-anxiety",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_svg_norm_hist((2, 2, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-voluntary",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_svg_norm_hist((2, 2, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strange-elder",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_svg_norm_hist((4, 4, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-navigator",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_svg_norm_hist((10, 10, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-truck",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_svg_norm_hist((10, 10, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-beast",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_svg_norm_hist((20, 20, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-joining",
   "metadata": {},
   "source": [
    "---\n",
    "## Evaluate cost scaling against each variable\n",
    "\n",
    "### Checklist\n",
    "- [x] Stack several LQGs including init dists\n",
    "- [x] Define range of `n_state`s, `n_ctrl`s, and `horizon`s\n",
    "- [x] Fix initial values of `n_state=2`, `n_ctrl=2`, `horizon=100`\n",
    "- [x] Iterate over one of the ranges, e.g., `horizon`s:\n",
    "  - [x] Generate several LQGs with the current `n_state`, `n_ctrl`, `horizon` and stack them\n",
    "  - [x] Find the optimal solutions (policies) and evaluate the optimal expected returns\n",
    "  - [x] Average the results accross the LQGs generated\n",
    "  - [x] Plot the average against the current `n_state`, `n_ctrl`, `horizon`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "speaking-stability",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_matrix(\n",
    "    state_dim_arr: np.ndarray,\n",
    "    ctrl_dim_arr: np.ndarray,\n",
    "    horizon_arr: np.ndarray,\n",
    "    perf_sampling_fn: Callable[[int, int, int], np.ndarray],\n",
    ") -> np.ndarray:\n",
    "    pbar = tqdm(\n",
    "        zip(state_dim_arr, ctrl_dim_arr, horizon_arr),\n",
    "        total=len(ctrl_dim_arr),\n",
    "        desc=\"Computing performance by LQG dims\",\n",
    "    )\n",
    "\n",
    "    exp_ret_arrs = [\n",
    "        perf_sampling_fn(n_state, n_ctrl, horizon) for n_state, n_ctrl, horizon in pbar\n",
    "    ]\n",
    "\n",
    "    exp_ret_mat = np.vstack(exp_ret_arrs)\n",
    "    return exp_ret_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-radar",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def average_performance_lineplot(\n",
    "    state_dim: np.ndarray,\n",
    "    ctrl_dim: np.ndarray,\n",
    "    horizon: np.ndarray,\n",
    "    x_axis: tuple[str, np.ndarray],\n",
    "):\n",
    "    x, x_arr = x_axis\n",
    "    y = \"average performance\"\n",
    "    hue = \"optimal\"\n",
    "    n_samples = 200\n",
    "\n",
    "    def sampler(s, c, h, *, optimal: bool):\n",
    "        return performance_samples((s, c, h), samples=n_samples, optimal=optimal)\n",
    "\n",
    "    optimal = {\n",
    "        x: x_arr,\n",
    "        y: performance_matrix(\n",
    "            state_dim, ctrl_dim, horizon, partial(sampler, optimal=True)\n",
    "        ).mean(-1),\n",
    "        hue: np.ones_like(x_arr, dtype=bool),\n",
    "    }\n",
    "    random = {\n",
    "        x: x_arr,\n",
    "        y: performance_matrix(\n",
    "            state_dim, ctrl_dim, horizon, partial(sampler, optimal=False)\n",
    "        ).mean(-1),\n",
    "        hue: np.zeros_like(x_arr, dtype=bool),\n",
    "    }\n",
    "\n",
    "    data = pd.concat(list(map(pd.DataFrame, (optimal, random))))\n",
    "    ax = sns.lineplot(x=x, y=y, data=data, hue=hue)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-conditions",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Performance vs. state dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portuguese-domestic",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = np.arange(2, 12)\n",
    "average_performance_lineplot(\n",
    "    state_dim=x_axis,\n",
    "    ctrl_dim=np.full(10, 2, dtype=int),\n",
    "    horizon=np.full(10, 100, dtype=int),\n",
    "    x_axis=(\"state dim\", x_axis),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funny-niger",
   "metadata": {},
   "source": [
    "#### Performance vs. control dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-success",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "state_dims = np.full(10, 2, dtype=int)\n",
    "ctrl_dims = np.arange(2, 12)\n",
    "horizons = np.full(10, 100, dtype=int)\n",
    "average_performance_lineplot(\n",
    "    state_dim=state_dims,\n",
    "    ctrl_dim=ctrl_dims,\n",
    "    horizon=horizons,\n",
    "    x_axis=(\"ctrl dim\", ctrl_dims),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-identity",
   "metadata": {},
   "source": [
    "#### Performance vs. horizon length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-myrtle",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "state_dims = np.full(10, 2, dtype=int)\n",
    "ctrl_dims = np.full(10, 2, dtype=int)\n",
    "horizons = 100 * np.arange(1, 11)\n",
    "average_performance_lineplot(state_dims, ctrl_dims, horizons, (\"horizon\", horizons))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-howard",
   "metadata": {},
   "source": [
    "#### Performance vs. state & control dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warming-flush",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "state_dims = ctrl_dims = np.arange(1, 21) + 1\n",
    "horizons = np.full(20, 100, dtype=int)\n",
    "average_performance_lineplot(\n",
    "    state_dims, ctrl_dims, horizons, (\"state & ctrl dim\", state_dims)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closing-comfort",
   "metadata": {},
   "source": [
    "---\n",
    "## Multivariate analysis of cost scaling\n",
    "\n",
    "### Checklist\n",
    "- [x] Plot the average expected return against two variables, e.g., `n_state` and `n_ctrl`, as a 3D surface plot\n",
    "- [ ] ~Use the insights gained from observing the cost scaling to propose a _downscaling_ parameter for random QuadCosts~\n",
    "- [x] Use the analysis above to also derive a scale parameter for the random Gaussian noise for policy generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-updating",
   "metadata": {},
   "outputs": [],
   "source": [
    "def large_fig(*args, scale: float = 2.0, **kwargs):\n",
    "    return plt.figure(*args, figsize=[scale * 6.4, scale * 4.8], **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-curtis",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_3d_from_2d_function(\n",
    "    ax,\n",
    "    xrange: np.ndarray,\n",
    "    yrange: np.ndarray,\n",
    "    function: callable[[np.ndarray, np.ndarray], np.ndarray],\n",
    "):\n",
    "    length = len(xrange)\n",
    "    xbatch, ybatch = np.meshgrid(xrange, yrange)\n",
    "    assert xbatch.shape == ybatch.shape == (length, length)\n",
    "\n",
    "    x_arr, y_arr = map(lambda x: x.reshape((-1,)), (xbatch, ybatch))\n",
    "    assert (\n",
    "        x_arr.shape == y_arr.shape == (length ** 2,)\n",
    "    ), f\"expected: {(length ** 2,)}; x_arr: {x_arr.shape}; y_arr {y_arr.shape}\"\n",
    "\n",
    "    z_arr = function(x_arr, y_arr)\n",
    "    assert z_arr.shape == x_arr.shape, f\"{z_arr.shape} != {x_arr.shape}\"\n",
    "\n",
    "    zbatch = z_arr.reshape(xbatch.shape)\n",
    "    ax.plot_surface(xbatch, ybatch, zbatch, cmap=cm.coolwarm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-outside",
   "metadata": {},
   "source": [
    "### State vs. control dimension with fixed horizon\n",
    "\n",
    "In what follows we set the horizon length to 100 steps.\n",
    "\n",
    "#### Best expected return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-sponsorship",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performances_by_state_ctrl_dim(\n",
    "    state_dim_arr: np.ndarray, ctrl_dim_arr: np.ndarray, horizon: int, optimal: bool\n",
    ") -> np.ndarray:\n",
    "    horizon_arr = np.full_like(state_dim_arr, fill_value=horizon)\n",
    "    exp_opt_ret_mat = performance_matrix(\n",
    "        state_dim_arr,\n",
    "        ctrl_dim_arr,\n",
    "        horizon_arr,\n",
    "        lambda s, c, h: performance_samples(\n",
    "            (s, c, h), optimal=optimal, samples=100, clip_ctrl_dim=False\n",
    "        ),\n",
    "    )\n",
    "    return exp_opt_ret_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-jungle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_performance_by_state_ctrl_dim(\n",
    "    state_dim_arr: np.ndarray, ctrl_dim_arr: np.ndarray, optimal: bool\n",
    ") -> np.ndarray:\n",
    "    z_arr = performances_by_state_ctrl_dim(\n",
    "        state_dim_arr, ctrl_dim_arr, horizon=100, optimal=optimal\n",
    "    ).mean(axis=-1)\n",
    "    # Filtering NaNs\n",
    "    z_arr = np.nan_to_num(z_arr, nan=np.nanmin(z_arr))\n",
    "    return z_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-capacity",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = large_fig()\n",
    "ax = fig.add_subplot(projection=\"3d\")\n",
    "plot_3d_from_2d_function(\n",
    "    ax,\n",
    "    xrange=np.arange(1, 21) + 1,\n",
    "    yrange=np.arange(1, 21) + 1,\n",
    "    function=lambda x, y: average_performance_by_state_ctrl_dim(x, y, optimal=True),\n",
    ")\n",
    "ax.set_xlabel(\"state dim\")\n",
    "ax.set_ylabel(\"ctrl dim\")\n",
    "ax.set_title(\"average best performance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miniature-backup",
   "metadata": {},
   "source": [
    "#### Expected return from random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-papua",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = large_fig()\n",
    "ax = fig.add_subplot(projection=\"3d\")\n",
    "plot_3d_from_2d_function(\n",
    "    ax,\n",
    "    xrange=np.arange(1, 21) + 1,\n",
    "    yrange=np.arange(1, 21) + 1,\n",
    "    function=lambda x, y: average_performance_by_state_ctrl_dim(x, y, optimal=False),\n",
    ")\n",
    "ax.set_xlabel(\"state dim\")\n",
    "ax.set_ylabel(\"ctrl dim\")\n",
    "ax.set_title(\"average random performance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-spank",
   "metadata": {},
   "source": [
    "### State & control dimension vs. horizon length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-handle",
   "metadata": {},
   "source": [
    "#### Best expected return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powerful-percentage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performances_by_horizon(\n",
    "    state_ctrl_dim: np.ndarray, horizon_arr: np.ndarray, optimal: bool\n",
    ") -> np.ndarray:\n",
    "    state_dim_arr = ctrl_dim_arr = state_ctrl_dim\n",
    "    opt_perf_mat = performance_matrix(\n",
    "        state_dim_arr,\n",
    "        ctrl_dim_arr,\n",
    "        horizon_arr,\n",
    "        lambda s, c, h: performance_samples((s, c, h), optimal=optimal, samples=100),\n",
    "    )\n",
    "    return opt_perf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coupled-kingston",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_performance_by_horizon(\n",
    "    state_ctrl_dim: np.ndarray, horizon_arr: np.ndarray, optimal: bool\n",
    ") -> np.ndarray:\n",
    "    z_arr = performances_by_horizon(state_ctrl_dim, horizon_arr, optimal=optimal).mean(\n",
    "        axis=-1\n",
    "    )\n",
    "    # Filtering NaNs\n",
    "    z_arr = np.nan_to_num(z_arr, nan=np.nanmin(z_arr))\n",
    "    return z_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-illness",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = large_fig()\n",
    "ax = fig.add_subplot(projection=\"3d\")\n",
    "plot_3d_from_2d_function(\n",
    "    ax,\n",
    "    xrange=np.arange(1, 21) + 1,\n",
    "    yrange=100 + 10 * np.arange(1, 21),\n",
    "    function=lambda x, y: average_performance_by_horizon(x, y, optimal=True),\n",
    ")\n",
    "ax.set_xlabel(\"state/ctrl dim\")\n",
    "ax.set_ylabel(\"horizon\")\n",
    "ax.set_title(\"average optimal performance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-omega",
   "metadata": {},
   "source": [
    "#### Random expected return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-italian",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = large_fig()\n",
    "ax = fig.add_subplot(projection=\"3d\")\n",
    "plot_3d_from_2d_function(\n",
    "    ax,\n",
    "    xrange=np.arange(1, 21) + 1,\n",
    "    yrange=100 + 10 * np.arange(1, 21),\n",
    "    function=lambda x, y: average_performance_by_horizon(x, y, optimal=False),\n",
    ")\n",
    "ax.set_xlabel(\"state/ctrl dim\")\n",
    "ax.set_ylabel(\"horizon\")\n",
    "ax.set_title(\"average random performance\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
