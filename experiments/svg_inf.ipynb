{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "monetary-double",
   "metadata": {},
   "source": [
    "# SVG(inf) for LQG\n",
    "\n",
    "## Introduction\n",
    "In this notebook we implement a simplified version of SVG($\\infty$) for the LQG problem. Our intention is to show how one can learn a parameterized policy in LQG via Stochastic Value Gradients. Note that even if we successfully learn a policy, we do not necessarily know what components of the SVG framework were instrumental in doing so. The focus of the rest of our work will be on analyzing the tenets of this framework based on **gradient estimation quality** and **performance curvature approximation**.\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portable-locking",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import itertools\n",
    "import os.path as osp\n",
    "from datetime import date\n",
    "from typing import Optional\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "from ray.rllib import RolloutWorker\n",
    "from ray.rllib import SampleBatch\n",
    "from ray.rllib.evaluation.metrics import collect_episodes, summarize_episodes\n",
    "from raylab.policy import TorchPolicy\n",
    "from raylab.policy.modules.actor import DeterministicPolicy\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import TensorDataset\n",
    "from tqdm.auto import trange\n",
    "\n",
    "import lqsvg.torch.named as nt\n",
    "from lqsvg.envs import lqr\n",
    "from lqsvg.envs.lqr.gym import TorchLQGMixin\n",
    "from lqsvg.policy.time_varying_linear import LQGPolicy\n",
    "\n",
    "from data import TrajectoryData  # pylint:disable=wrong-import-order\n",
    "from models import MonteCarloSVG  # pylint:disable=wrong-import-order\n",
    "from models import glorot_init_model  # pylint:disable=wrong-import-order\n",
    "from policy import make_worker  # pylint:disable=wrong-import-order\n",
    "from tqdm_util import collect_with_progress  # pylint:disable=wrong-import-order\n",
    "from utils import group_batch_episodes  # pylint:disable=wrong-import-order\n",
    "from utils import suppress_dataloader_warning  # pylint:disable=wrong-import-order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-connection",
   "metadata": {},
   "source": [
    "---\n",
    "## Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-stadium",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(rollout_worker: RolloutWorker, hparams: dict) -> pl.LightningDataModule:\n",
    "    del rollout_worker\n",
    "    return DataModule(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-ethics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(policy: DeterministicPolicy, hparams: dict) -> Optimizer:\n",
    "    return torch.optim.Adam(policy.parameters(), lr=hparams[\"policy_lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-season",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_trajs(rollout_worker: RolloutWorker, dataset: pl.LightningDataModule, hparams: dict):\n",
    "    dataset.collect_trajectories(rollout_worker, n_trajs=hparams[\"trajs_per_iter\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "american-camping",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(model: pl.LightningModule, dataset: pl.LightningDataModule, run):\n",
    "    hparams = run.config\n",
    "    early_stopping = pl.callbacks.EarlyStopping(\n",
    "        model.early_stop_on,\n",
    "        min_delta=hparams[\"improvement_delta\"],\n",
    "        patience=hparams[\"patience\"],\n",
    "        mode=\"min\",\n",
    "        strict=True,\n",
    "    )\n",
    "    trainer = pl.Trainer(callbacks=[early_stopping], max_epochs=1000, progress_bar_refresh_rate=0)\n",
    "    with suppress_dataloader_warning():\n",
    "        trainer.fit(model, datamodule=dataset)\n",
    "\n",
    "    # Logging\n",
    "    run.log({\"model_epochs\": trainer.current_epoch + 1}, commit=False)\n",
    "    print(\"Model epochs:\", trainer.current_epoch + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-exchange",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_policy(policy: DeterministicPolicy, model: pl.LightningModule, optimizer: Optimizer, hparams: dict):\n",
    "    svg = MonteCarloSVG(policy, model.model)\n",
    "    optimizer.zero_grad()\n",
    "    loss = -svg.value(hparams[\"svg_samples\"])\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-irrigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svg_inf(policy: DeterministicPolicy, model: pl.LightningModule, rollout_worker: RolloutWorker, run):    \n",
    "    artifact = wandb.Artifact(f\"svg_inf-lqg{model.model.n_state}.{model.model.n_ctrl}.{model.model.horizon}\", type=\"model\")\n",
    "    collect_metrics = CollectMetrics()\n",
    "\n",
    "    ### Main algorithm logic ###\n",
    "    dataset = create_dataset(rollout_worker, run.config)\n",
    "    optimizer = create_optimizer(policy, run.config)\n",
    "    for itr in trange(hparams[\"iterations\"], desc=\"SVG(inf) iteration\"):\n",
    "        collect_trajs(rollout_worker, dataset, run.config)\n",
    "        optimize_model(model, dataset, run)\n",
    "        step_policy(policy, model, optimizer, run.config)\n",
    "    ############################\n",
    "        \n",
    "        # Logging\n",
    "        run.log({\"svg_iter\": iter}, commit=False)\n",
    "        run.log(collect_metrics(rollout_worker))\n",
    "        \n",
    "        if itr % 10 == 0 or itr == hparams[\"iterations\"] - 1:\n",
    "            add_ckpt_to_artifact(itr + 1, artifact, rollout_worker.get_policy(), run)\n",
    "            \n",
    "    run.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-ebony",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ckpt_to_artifact(itr: int, artifact: wandb.Artifact, policy: TorchPolicy, run):\n",
    "    path = osp.join(run.dir, f\"module_{itr}.pt\")\n",
    "    torch.save(policy.module.state_dict(), path)\n",
    "    artifact.add_file(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entertaining-choice",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personalized-conspiracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollectMetrics:\n",
    "    def __init__(self):\n",
    "        self.episode_history = []\n",
    "        self.to_be_collected = []\n",
    "\n",
    "    def __call__(self, worker: RolloutWorker, min_history: int = 100, timeout_seconds: int = 180) -> dict:\n",
    "        # Collect worker metrics.\n",
    "        episodes, self.to_be_collected = collect_episodes(\n",
    "            worker, to_be_collected=self.to_be_collected, timeout_seconds=timeout_seconds\n",
    "        )\n",
    "        orig_episodes = list(episodes)\n",
    "        missing = min_history - len(episodes)\n",
    "        if missing > 0:\n",
    "            episodes.extend(self.episode_history[-missing:])\n",
    "            assert len(episodes) <= min_history\n",
    "        self.episode_history.extend(orig_episodes)\n",
    "        self.episode_history = self.episode_history[-min_history:]\n",
    "        return summarize_episodes(episodes, orig_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-split",
   "metadata": {},
   "source": [
    "### Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-florence",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, hparams: dict):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_size: float = hparams[\"dataset_batch_size\"]\n",
    "        self.train_val_split: tuple[float, float] = hparams[\"train_val_split\"]\n",
    "\n",
    "        self.full_dataset = None\n",
    "        self.train_dataset, self.val_dataset = None, None\n",
    "        self._itr_datasets: list[TensorDataset] = []\n",
    "        \n",
    "    def collect_trajectories(self, rollout_worker: RolloutWorker, n_trajs: int):\n",
    "        worker = rollout_worker\n",
    "        self._check_rollout_worker(worker)\n",
    "        \n",
    "        sample_batch = collect_with_progress(worker, n_trajs, prog=False)\n",
    "        sample_batch = group_batch_episodes(sample_batch)\n",
    "        trajs = sample_batch.split_by_episode()\n",
    "        self._check_collected_trajs(trajs, worker, n_trajs)\n",
    "        \n",
    "        traj_dataset = TrajectoryData.trajectory_dataset(trajs)\n",
    "        self._itr_datasets += [traj_dataset]\n",
    "        self.full_dataset = ConcatDataset(self._itr_datasets)\n",
    "        \n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        del stage\n",
    "        train_frac, _ = self.train_val_split\n",
    "        train_trajs = int(train_frac * len(self.full_dataset))\n",
    "        val_trajs = len(self.full_dataset) - train_trajs\n",
    "\n",
    "        self.train_dataset, self.val_dataset = random_split(self.full_dataset, (train_trajs, val_trajs))\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        # pylint:disable=arguments-differ\n",
    "        return DataLoader(self.train_dataset, shuffle=True, batch_size=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        # pylint:disable=arguments-differ\n",
    "        return DataLoader(self.val_dataset, shuffle=False, batch_size=self.batch_size)\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_rollout_worker(worker: RolloutWorker):\n",
    "        assert worker.rollout_fragment_length == worker.env.horizon * worker.num_envs\n",
    "        assert worker.batch_mode == \"truncate_episodes\"\n",
    "        \n",
    "    @staticmethod\n",
    "    def _check_collected_trajs(trajs: list[SampleBatch], worker: RolloutWorker, n_trajs: int):\n",
    "        traj_counts = [t.count for t in trajs]\n",
    "        assert all(c == worker.env.horizon for c in traj_counts), traj_counts\n",
    "        total_ts = sum(t.count for t in trajs)\n",
    "        assert total_ts == n_trajs * worker.env.horizon, total_ts        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-occurrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvModel(pl.LightningModule):\n",
    "    early_stop_on: str = \"val/loss\"\n",
    "\n",
    "    def __init__(self, policy: LQGPolicy, hparams: dict):\n",
    "        super().__init__()\n",
    "        self.model = policy.module.model\n",
    "\n",
    "        self.hparams.learning_rate = hparams[\"model_lr\"]\n",
    "        glorot_init_model(self.model)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        params = nn.ParameterList(\n",
    "            itertools.chain(self.model.trans.parameters(), self.model.init.parameters())\n",
    "        )\n",
    "        optim = torch.optim.Adam(params, lr=self.hparams.learning_rate)\n",
    "        return optim\n",
    "\n",
    "    def forward(self, obs: Tensor, act: Tensor, new_obs: Tensor) -> Tensor:\n",
    "        \"\"\"Batched trajectory log prob.\"\"\"\n",
    "        # pylint:disable=arguments-differ\n",
    "        return self.model.log_prob(obs, act, new_obs)\n",
    "\n",
    "    def _compute_loss_on_batch(\n",
    "        self, batch: tuple[Tensor, Tensor, Tensor], batch_idx: int\n",
    "    ) -> Tensor:\n",
    "        del batch_idx\n",
    "        obs, act, new_obs = (x.refine_names(\"B\", \"H\", \"R\") for x in batch)\n",
    "        return -self(obs, act, new_obs).mean()\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: tuple[Tensor, Tensor, Tensor], batch_idx: int\n",
    "    ) -> Tensor:\n",
    "        # pylint:disable=arguments-differ\n",
    "        loss = self._compute_loss_on_batch(batch, batch_idx)\n",
    "        self.log(\"train/loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(\n",
    "        self, batch: tuple[Tensor, Tensor, Tensor], batch_idx: int\n",
    "    ) -> Tensor:\n",
    "        # pylint:disable=arguments-differ\n",
    "        loss = self._compute_loss_on_batch(batch, batch_idx)\n",
    "        self.log(\"val/loss\", loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-constraint",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-space",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calver() -> str:\n",
    "    today = date.today()\n",
    "    return f\"{today.month}.{today.day}.0\"\n",
    "\n",
    "print(\"CalVer:\", calver())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-puppy",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    name=\"SVG(inf)\",\n",
    "    project=\"LQG-SVG\",\n",
    "    entity=\"angelovtt\",\n",
    "    tags=[calver()],\n",
    "    mode=\"online\",\n",
    "    save_code=True,\n",
    "    job_type=\"train\",\n",
    ")\n",
    "\n",
    "with run, nt.suppress_named_tensor_warning():\n",
    "    env_config = dict(n_state=2, n_ctrl=2, horizon=100, num_envs=100)\n",
    "    hparams = {\n",
    "        \"iterations\": 50,\n",
    "        \"trajs_per_iter\": 2000,\n",
    "        \"policy_lr\": 3e-4,\n",
    "        \"improvement_delta\": 0.0,\n",
    "        \"patience\": 3,\n",
    "        \"svg_samples\": 32,\n",
    "        \"dataset_batch_size\": 64,\n",
    "        \"train_val_split\": (0.8, 0.2),\n",
    "        \"model_lr\": 1e-3,\n",
    "        \"env_config\": env_config,\n",
    "    }\n",
    "    \n",
    "    run.config.update(hparams)\n",
    "    worker = make_worker(env_config)\n",
    "    rllib_policy = worker.get_policy()\n",
    "    policy = rllib_policy.module.actor\n",
    "    model = EnvModel(rllib_policy, run.config)\n",
    "    \n",
    "    svg_inf(policy, model, worker, run)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
