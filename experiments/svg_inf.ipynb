{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "monthly-posting",
   "metadata": {},
   "source": [
    "# SVG(inf) for LQG\n",
    "\n",
    "## Introduction\n",
    "In this notebook we implement a simplified version of SVG($\\infty$) for the LQG problem. Our intention is to show how one can learn a parameterized policy in LQG via Stochastic Value Gradients. Note that even if we successfully learn a policy, we do not necessarily know what components of the SVG framework were instrumental in doing so. The focus of the rest of our work will be on analyzing the tenets of this framework based on **gradient estimation quality** and **performance curvature approximation**.\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "latest-horizon",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import logging\n",
    "import itertools\n",
    "import os.path as osp\n",
    "from numbers import Number\n",
    "from typing import Optional\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import ray\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "from ray.rllib import RolloutWorker\n",
    "from ray.rllib import SampleBatch\n",
    "from ray.rllib.evaluation.metrics import collect_episodes, summarize_episodes\n",
    "from raylab.policy import TorchPolicy\n",
    "from raylab.policy.modules.actor import DeterministicPolicy\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import TensorDataset\n",
    "from tqdm.auto import trange\n",
    "\n",
    "import lqsvg.torch.named as nt\n",
    "from lqsvg.envs import lqr\n",
    "from lqsvg.envs.lqr.gym import TorchLQGMixin\n",
    "from lqsvg.experiment.data import TrajectoryData\n",
    "from lqsvg.experiment.models import MonteCarloSVG\n",
    "from lqsvg.experiment.models import glorot_init_model\n",
    "from lqsvg.experiment.policy import make_worker\n",
    "from lqsvg.experiment.tqdm_util import collect_with_progress\n",
    "from lqsvg.experiment.utils import calver\n",
    "from lqsvg.experiment.utils import group_batch_episodes\n",
    "from lqsvg.experiment.utils import linear_feedback_distance\n",
    "from lqsvg.experiment.utils import suppress_dataloader_warning\n",
    "from lqsvg.policy.time_varying_linear import LQGPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-agenda",
   "metadata": {},
   "source": [
    "---\n",
    "## Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "universal-third",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class Experiment:\n",
    "    def __init__(self, config: dict):\n",
    "        ver = calver()\n",
    "        self.run = wandb.init(\n",
    "            job_type=\"train\",\n",
    "            config=config,\n",
    "            project=\"LQG-SVG\",\n",
    "            entity=\"angelovtt\",\n",
    "            reinit=True,\n",
    "            tags=[ver],\n",
    "            name=\"SVG(inf)\",\n",
    "            mode=\"offline\",\n",
    "            allow_val_change=False,\n",
    "            save_code=True,\n",
    "        )\n",
    "\n",
    "        print(\"CalVer:\", ver)\n",
    "        self.worker = None\n",
    "        self.policy = None\n",
    "        self.model = None\n",
    "        self.artifact = None\n",
    "        self.collect_metrics = None\n",
    "        \n",
    "    @property\n",
    "    def dir(self) -> str:\n",
    "        return self.run.dir\n",
    "    \n",
    "    @property\n",
    "    def hparams(self) -> dict:\n",
    "        return self.run.config\n",
    "        \n",
    "    def setup(self):\n",
    "        with nt.suppress_named_tensor_warning():\n",
    "            self.worker = make_worker(self.hparams.env_config)\n",
    "\n",
    "        rllib_policy = self.worker.get_policy()\n",
    "        self.policy = rllib_policy.module.actor\n",
    "        self.model = EnvModel(rllib_policy, self.hparams)\n",
    "        if self.hparams.true_model:\n",
    "            self.model.model = self.worker.env.module\n",
    "        \n",
    "        self.artifact = self.create_artifact()\n",
    "        self.save_mdp_to_artifact()\n",
    "        self.collect_metrics = CollectMetrics()\n",
    "        self.suppress_lightning_info_logging()\n",
    "        \n",
    "    @staticmethod\n",
    "    def suppress_lightning_info_logging():\n",
    "        # https://github.com/PyTorchLightning/pytorch-lightning/issues/3431\n",
    "        logging.getLogger('lightning').setLevel(logging.WARNING)\n",
    "        \n",
    "    def create_artifact(self) -> wandb.Artifact:\n",
    "        env = self.worker.env\n",
    "        return wandb.Artifact(f\"svg_inf-lqg{env.n_state}.{env.n_ctrl}.{env.horizon}\", type=\"model\")\n",
    "    \n",
    "    def save_mdp_to_artifact(self):\n",
    "        mdp = self.worker.env.module\n",
    "        path = osp.join(self.dir, \"mdp.pt\")\n",
    "        torch.save(mdp.state_dict(), path)\n",
    "        self.artifact.add_file(path)        \n",
    "\n",
    "    def execute(self):\n",
    "        \"\"\"Train and finish run.\"\"\"\n",
    "        self.train()\n",
    "        self.finish()\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Main algorithm logic.\"\"\"\n",
    "        dataset = self.create_dataset()\n",
    "        optimizer = self.create_optimizer(self.policy)\n",
    "\n",
    "        for itr in trange(self.hparams.iterations, desc=\"SVG(inf)\", unit=\"iteration\", disable=True):\n",
    "            if itr % 10 == 0:\n",
    "                self.add_ckpt_to_artifact(itr)\n",
    "\n",
    "            self.collect_trajs(self.worker, dataset)\n",
    "            self.optimize_model(self.model, dataset)\n",
    "            self.step_policy(self.policy, self.model, optimizer)\n",
    "\n",
    "            # Logging\n",
    "            self.log_iteration(itr)\n",
    "        \n",
    "    def create_dataset(self) -> pl.LightningDataModule:\n",
    "        return DataModule(self.hparams)\n",
    "    \n",
    "    def create_optimizer(self, policy: nn.Module) -> Optimizer:\n",
    "        return torch.optim.Adam(policy.parameters(), lr=self.hparams[\"policy_lr\"])\n",
    "    \n",
    "    def collect_trajs(self, worker: RolloutWorker, dataset: pl.LightningDataModule):\n",
    "        dataset.collect_trajectories(worker, n_trajs=self.hparams[\"trajs_per_iter\"])\n",
    "\n",
    "    def optimize_model(self, model: pl.LightningModule, dataset: pl.LightningDataModule):\n",
    "        if self.hparams.true_model:\n",
    "            return\n",
    "\n",
    "        hparams = self.hparams\n",
    "\n",
    "        validation_results = ValidationResults()\n",
    "        early_stopping = pl.callbacks.EarlyStopping(\n",
    "            model.early_stop_on,\n",
    "            min_delta=hparams[\"improvement_delta\"],\n",
    "            patience=hparams[\"patience\"],\n",
    "            mode=\"min\",\n",
    "            strict=True,\n",
    "        )\n",
    "        trainer = pl.Trainer(\n",
    "            default_root_dir=self.dir,\n",
    "            callbacks=[early_stopping, validation_results], \n",
    "            max_epochs=1000, \n",
    "            progress_bar_refresh_rate=0,  # don't show progress bar for model training\n",
    "            weights_summary=None,  # don't print summary before training\n",
    "            checkpoint_callback=False,  # don't save last model\n",
    "        )\n",
    "        with suppress_dataloader_warning():\n",
    "            trainer.fit(model, datamodule=dataset)\n",
    "\n",
    "        # Logging\n",
    "        results = {\n",
    "            \"model_epochs\": trainer.current_epoch + 1, \n",
    "            \"model_nll\": validation_results.last_validation_loss().item(),\n",
    "        }\n",
    "        self.run.log(results, commit=False)\n",
    "\n",
    "    def step_policy(self, policy: DeterministicPolicy, model: pl.LightningModule, optimizer: Optimizer):\n",
    "        svg = MonteCarloSVG(policy, model.model)\n",
    "        optimizer.zero_grad()\n",
    "        loss = -svg.value(self.hparams[\"svg_samples\"])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    def log_iteration(self, itr: int):\n",
    "        pistar, _, _ = self.worker.env.solution\n",
    "        logs = {\n",
    "            \"iteration\": itr, \n",
    "            \"distance_to_optimal\": linear_feedback_distance(self.policy.standard_form(), pistar), \n",
    "            **self.collect_metrics(self.worker)\n",
    "        }\n",
    "        self.run.log(logs)\n",
    "        \n",
    "    def finish(self):\n",
    "        self.add_ckpt_to_artifact(self.hparams[\"iterations\"])            \n",
    "        self.run.log_artifact(self.artifact)\n",
    "        self.run.finish()\n",
    "\n",
    "    def add_ckpt_to_artifact(self, itr: int):\n",
    "        policy: TorchPolicy = self.worker.get_policy()\n",
    "        path = osp.join(self.dir, f\"module-iter={itr}.pt\")\n",
    "        torch.save(policy.module.state_dict(), path)\n",
    "        self.artifact.add_file(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-overhead",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bulgarian-boards",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollectMetrics:\n",
    "    # Copied from https://github.com/ray-project/ray/blob/c409b5b63a6928e423428b700e528e35d791e8ea/rllib/execution/metric_ops.py#L47\n",
    "    def __init__(self):\n",
    "        self.episode_history = []\n",
    "        self.to_be_collected = []\n",
    "\n",
    "    def __call__(self, worker: RolloutWorker, min_history: int = 100, timeout_seconds: int = 180) -> dict:\n",
    "        # Collect worker metrics.\n",
    "        episodes, self.to_be_collected = collect_episodes(\n",
    "            worker, to_be_collected=self.to_be_collected, timeout_seconds=timeout_seconds\n",
    "        )\n",
    "        orig_episodes = list(episodes)\n",
    "        missing = min_history - len(episodes)\n",
    "        if missing > 0:\n",
    "            episodes.extend(self.episode_history[-missing:])\n",
    "            assert len(episodes) <= min_history\n",
    "        self.episode_history.extend(orig_episodes)\n",
    "        self.episode_history = self.episode_history[-min_history:]\n",
    "        return {k: v for k, v in summarize_episodes(episodes, orig_episodes).items() if isinstance(v, Number)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "younger-tongue",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationResults(pl.callbacks.Callback):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.saved_outputs = None\n",
    "\n",
    "    def on_validation_epoch_start(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n",
    "        self.saved_outputs: list[Tensor] = []\n",
    "            \n",
    "    def on_validation_batch_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule, outputs: Tensor, *args, **kwargs):\n",
    "        self.saved_outputs += [outputs]\n",
    "        \n",
    "    def last_validation_loss(self) -> Tensor:\n",
    "        return torch.stack(self.saved_outputs, dim=0).mean(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-covering",
   "metadata": {},
   "source": [
    "### Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "polish-channel",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, hparams: dict):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_size: float = hparams[\"dataset_batch_size\"]\n",
    "        self.train_val_split: tuple[float, float] = hparams[\"train_val_split\"]\n",
    "\n",
    "        self.full_dataset = None\n",
    "        self.train_dataset, self.val_dataset = None, None\n",
    "        self._itr_datasets: list[TensorDataset] = []\n",
    "        \n",
    "    def collect_trajectories(self, rollout_worker: RolloutWorker, n_trajs: int):\n",
    "        worker = rollout_worker\n",
    "        self._check_rollout_worker(worker)\n",
    "        \n",
    "        sample_batch = collect_with_progress(worker, n_trajs, prog=False)\n",
    "        sample_batch = group_batch_episodes(sample_batch)\n",
    "        trajs = sample_batch.split_by_episode()\n",
    "        self._check_collected_trajs(trajs, worker, n_trajs)\n",
    "        \n",
    "        traj_dataset = TrajectoryData.trajectory_dataset(trajs)\n",
    "        self._itr_datasets += [traj_dataset]\n",
    "        self.full_dataset = ConcatDataset(self._itr_datasets)\n",
    "        \n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        del stage\n",
    "        train_frac, _ = self.train_val_split\n",
    "        train_trajs = int(train_frac * len(self.full_dataset))\n",
    "        val_trajs = len(self.full_dataset) - train_trajs\n",
    "\n",
    "        self.train_dataset, self.val_dataset = random_split(self.full_dataset, (train_trajs, val_trajs))\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        # pylint:disable=arguments-differ\n",
    "        return DataLoader(self.train_dataset, shuffle=True, batch_size=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        # pylint:disable=arguments-differ\n",
    "        return DataLoader(self.val_dataset, shuffle=False, batch_size=self.batch_size)\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_rollout_worker(worker: RolloutWorker):\n",
    "        assert worker.rollout_fragment_length == worker.env.horizon * worker.num_envs\n",
    "        assert worker.batch_mode == \"truncate_episodes\"\n",
    "        \n",
    "    @staticmethod\n",
    "    def _check_collected_trajs(trajs: list[SampleBatch], worker: RolloutWorker, n_trajs: int):\n",
    "        traj_counts = [t.count for t in trajs]\n",
    "        assert all(c == worker.env.horizon for c in traj_counts), traj_counts\n",
    "        total_ts = sum(t.count for t in trajs)\n",
    "        assert total_ts == n_trajs * worker.env.horizon, total_ts        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "armed-nightmare",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvModel(pl.LightningModule):\n",
    "    early_stop_on: str = \"val/loss\"\n",
    "\n",
    "    def __init__(self, policy: LQGPolicy, hparams: dict):\n",
    "        super().__init__()\n",
    "        self.model = policy.module.model\n",
    "\n",
    "        self.hparams.learning_rate = hparams[\"model_lr\"]\n",
    "        glorot_init_model(self.model)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        params = nn.ParameterList(\n",
    "            itertools.chain(self.model.trans.parameters(), self.model.init.parameters())\n",
    "        )\n",
    "        optim = torch.optim.Adam(params, lr=self.hparams.learning_rate)\n",
    "        return optim\n",
    "\n",
    "    def forward(self, obs: Tensor, act: Tensor, new_obs: Tensor) -> Tensor:\n",
    "        \"\"\"Batched trajectory log prob.\"\"\"\n",
    "        # pylint:disable=arguments-differ\n",
    "        return self.model.log_prob(obs, act, new_obs)\n",
    "\n",
    "    def _compute_loss_on_batch(\n",
    "        self, batch: tuple[Tensor, Tensor, Tensor], batch_idx: int\n",
    "    ) -> Tensor:\n",
    "        del batch_idx\n",
    "        obs, act, new_obs = (x.refine_names(\"B\", \"H\", \"R\") for x in batch)\n",
    "        return -self(obs, act, new_obs).mean()\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: tuple[Tensor, Tensor, Tensor], batch_idx: int\n",
    "    ) -> Tensor:\n",
    "        # pylint:disable=arguments-differ\n",
    "        loss = self._compute_loss_on_batch(batch, batch_idx)\n",
    "        self.log(\"train/loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(\n",
    "        self, batch: tuple[Tensor, Tensor, Tensor], batch_idx: int\n",
    "    ) -> Tensor:\n",
    "        # pylint:disable=arguments-differ\n",
    "        loss = self._compute_loss_on_batch(batch, batch_idx)\n",
    "        self.log(\"val/loss\", loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-sequence",
   "metadata": {},
   "source": [
    "---\n",
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "revised-performer",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-19 09:18:44,363\tINFO services.py:1172 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb: Currently logged in as: angelovtt (use `wandb login --relogin` to force relogin)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: Currently logged in as: angelovtt (use `wandb login --relogin` to force relogin)\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb: Currently logged in as: angelovtt (use `wandb login --relogin` to force relogin)\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb: Currently logged in as: angelovtt (use `wandb login --relogin` to force relogin)\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb: ERROR Error while calling W&B API: Error 1213: Deadlock found when trying to get lock; try restarting transaction (<Response [500]>)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: Tracking run with wandb version 0.10.22\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: Syncing run SVG(inf)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/angelovtt/LQG-SVG\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: 🚀 View run at https://wandb.ai/angelovtt/LQG-SVG/runs/1u5x3tbs\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: Run data is saved locally in /Users/angelolovatto/Repositories/personal/LQG-SVG/experiments/wandb/run-20210319_091853-1u5x3tbs\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m 2021-03-19 09:18:56,329\tINFO rollout_worker.py:1114 -- Built policy map: {'default_policy': LQGPolicy(\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m   Box(-inf, inf, (3,), float32),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m   Box(-inf, inf, (2,), float32),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m   {\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m     compile: false\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m     env_config: {}\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m     exploration_config:\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m       pure_exploration_steps: 0\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m       type: raylab.utils.exploration.GaussianNoise\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m     explore: true\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m     framework: torch\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m     gamma: 0.99\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m     module: {}\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m     num_workers: 0\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m     optimizer: {}\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m     worker_index: 0\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m   }\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m )}\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m 2021-03-19 09:18:56,329\tINFO rollout_worker.py:1115 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x17f139100>}\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m 2021-03-19 09:18:56,329\tINFO rollout_worker.py:526 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x17f3b4100>}\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb: Tracking run with wandb version 0.10.22\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb: Syncing run SVG(inf)\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/angelovtt/LQG-SVG\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb: 🚀 View run at https://wandb.ai/angelovtt/LQG-SVG/runs/2y5fpjmo\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb: Run data is saved locally in /Users/angelolovatto/Repositories/personal/LQG-SVG/experiments/wandb/run-20210319_091853-2y5fpjmo\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m 2021-03-19 09:18:56,390\tINFO rollout_worker.py:1114 -- Built policy map: {'default_policy': LQGPolicy(\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m   Box(-inf, inf, (3,), float32),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m   Box(-inf, inf, (2,), float32),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m   {\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m     compile: false\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m     env_config: {}\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m     exploration_config:\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m       pure_exploration_steps: 0\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m       type: raylab.utils.exploration.GaussianNoise\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m     explore: true\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m     framework: torch\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m     gamma: 0.99\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m     module: {}\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m     num_workers: 0\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m     optimizer: {}\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m     worker_index: 0\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m   }\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m )}\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m 2021-03-19 09:18:56,391\tINFO rollout_worker.py:1115 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x175ecafa0>}\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m 2021-03-19 09:18:56,391\tINFO rollout_worker.py:526 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x175ecaf70>}\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m 2021-03-19 09:18:56,421\tINFO rollout_worker.py:659 -- Generating sample batch of size 2000\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m 2021-03-19 09:18:56,428\tINFO sampler.py:593 -- Raw obs from env: { 0: { 'agent0': np.ndarray((3,), dtype=float32, min=0.0, max=0.194, mean=0.069)},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m   1: { 'agent0': np.ndarray((3,), dtype=float32, min=-0.545, max=0.842, mean=0.099)},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m   2: { 'agent0': np.ndarray((3,), dtype=float32, min=-1.511, max=0.0, mean=-0.593)},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m   3: { 'agent0': np.ndarray((3,), dtype=float32, min=0.0, max=0.851, mean=0.401)},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m   4: { 'agent0': np.ndarray((3,), dtype=float32, min=0.0, max=2.003, mean=0.961)},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m   5: { 'agent0': np.ndarray((3,), dtype=float32, min=-2.243, max=0.55, mean=-0.564)},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m   6: { 'agent0': np.ndarray((3,), dtype=float32, min=0.0, max=1.978, mean=0.991)},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m   7: { 'agent0': np.ndarray((3,), dtype=float32, min=-1.166, max=1.432, mean=0.089)},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m   8: { 'agent0': np.ndarray((3,), dtype=float32, min=0.0, max=1.881, mean=0.838)},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m   9: { 'agent0': np.ndarray((3,), dtype=float32, min=-0.509, max=0.0, mean=-0.239)},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m   10: { 'agent0': np.ndarray((3,), dtype=float32, min=-0.29, max=0.0, mean=-0.161)},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m   11: { 'agent0': np.ndarray((3,), dtype=float32, min=-0.563, max=0.794, mean=0.077)},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m   12: { 'agent0': np.ndarray((3,), dtype=float32, min=-0.65, max=0.663, mean=0.004)},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m   13: { 'agent0': np.ndarray((3,), dtype=float32, min=-0.443, max=0.863, mean=0.14)},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m   14: { 'agent0': np.ndarray((3,), dtype=float32, min=-0.585, max=0.338, mean=-0.082)},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m   15: { 'agent0': np.ndarray((3,), dtype=float32, min=-1.155, max=0.0, mean=-0.43)},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m   16: { 'agent0': np.ndarray((3,), dtype=float32, min=-0.653, max=0.0, mean=-0.26)},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m   17: { 'agent0': np.ndarray((3,), dtype=float32, min=-0.009, max=1.099, mean=0.363)},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m   18: { 'agent0': np.ndarray((3,), dtype=float32, min=-0.07, max=0.776, mean=0.235)},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m   19: { 'agent0': np.ndarray((3,), dtype=float32, min=0.0, max=1.533, mean=0.511)}}\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m 2021-03-19 09:18:56,430\tINFO sampler.py:595 -- Info return from env: { 0: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m   1: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m   2: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m   3: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m   4: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m   5: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m   6: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m   7: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m   8: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m   9: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m   10: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m   11: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m   12: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m   13: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m   14: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m   15: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m   16: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m   17: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m   18: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m   19: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m 2021-03-19 09:18:56,431\tWARNING deprecation.py:33 -- DeprecationWarning: `env_index` has been deprecated. Use `episode.env_id` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m 2021-03-19 09:18:56,431\tINFO sampler.py:835 -- Preprocessed obs: np.ndarray((3,), dtype=float32, min=0.0, max=0.194, mean=0.069)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m 2021-03-19 09:18:56,432\tINFO sampler.py:840 -- Filtered obs: np.ndarray((3,), dtype=float32, min=0.0, max=0.194, mean=0.069)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m 2021-03-19 09:18:56,455\tINFO sampler.py:1249 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=0.0, max=0.194, mean=0.069),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'env_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-0.545, max=0.842, mean=0.099),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'env_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-1.511, max=0.0, mean=-0.593),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'env_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=0.0, max=0.851, mean=0.401),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'env_id': 4,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=0.0, max=2.003, mean=0.961),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'env_id': 5,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-2.243, max=0.55, mean=-0.564),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'env_id': 6,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=0.0, max=1.978, mean=0.991),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb: Tracking run with wandb version 0.10.22\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb: Syncing run SVG(inf)\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/angelovtt/LQG-SVG\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb: 🚀 View run at https://wandb.ai/angelovtt/LQG-SVG/runs/3dz43gtk\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb: Run data is saved locally in /Users/angelolovatto/Repositories/personal/LQG-SVG/experiments/wandb/run-20210319_091853-3dz43gtk\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m 2021-03-19 09:18:56,455\tINFO rollout_worker.py:659 -- Generating sample batch of size 2000\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'env_id': 7,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-1.166, max=1.432, mean=0.089),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'env_id': 8,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=0.0, max=1.881, mean=0.838),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'env_id': 9,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-0.509, max=0.0, mean=-0.239),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'env_id': 10,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-0.29, max=0.0, mean=-0.161),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'env_id': 11,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-0.563, max=0.794, mean=0.077),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'env_id': 12,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-0.65, max=0.663, mean=0.004),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'env_id': 13,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-0.443, max=0.863, mean=0.14),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'env_id': 14,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-0.585, max=0.338, mean=-0.082),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'env_id': 15,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-1.155, max=0.0, mean=-0.43),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'env_id': 16,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-0.653, max=0.0, mean=-0.26),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'env_id': 17,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-0.009, max=1.099, mean=0.363),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'env_id': 18,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-0.07, max=0.776, mean=0.235),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'env_id': 19,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=0.0, max=1.533, mean=0.511),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m CalVer: 3.19.0\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m CalVer: 3.19.0\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m CalVer: 3.19.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m 2021-03-19 09:18:56,492\tINFO sampler.py:1295 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m { 'default_policy': ( np.ndarray((20, 2), dtype=float32, min=-4.32, max=-0.305, mean=-1.847),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                       {})}\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m 2021-03-19 09:18:56,480\tINFO rollout_worker.py:1114 -- Built policy map: {'default_policy': LQGPolicy(\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m   Box(-inf, inf, (3,), float32),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m   Box(-inf, inf, (2,), float32),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m   {\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m     compile: false\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m     env_config: {}\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m     exploration_config:\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m       pure_exploration_steps: 0\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m       type: raylab.utils.exploration.GaussianNoise\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m     explore: true\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m     framework: torch\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m     gamma: 0.99\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m     module: {}\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m     num_workers: 0\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m     optimizer: {}\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m     worker_index: 0\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m   }\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m )}\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m 2021-03-19 09:18:56,480\tINFO rollout_worker.py:1115 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x1849ff040>}\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m 2021-03-19 09:18:56,480\tINFO rollout_worker.py:526 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x1849f2fd0>}\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m 2021-03-19 09:18:56,469\tINFO sampler.py:593 -- Raw obs from env: { 0: { 'agent0': np.ndarray((3,), dtype=float32, min=-0.93, max=1.511, mean=0.194)},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m   1: { 'agent0': np.ndarray((3,), dtype=float32, min=-0.084, max=1.054, mean=0.323)},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m   2: { 'agent0': np.ndarray((3,), dtype=float32, min=0.0, max=0.553, mean=0.272)},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m   3: { 'agent0': np.ndarray((3,), dtype=float32, min=-0.916, max=0.0, mean=-0.415)},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m   4: { 'agent0': np.ndarray((3,), dtype=float32, min=-0.551, max=0.647, mean=0.032)},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m   5: { 'agent0': np.ndarray((3,), dtype=float32, min=-1.421, max=1.746, mean=0.108)},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m   6: { 'agent0': np.ndarray((3,), dtype=float32, min=-1.815, max=0.0, mean=-0.934)},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m   7: { 'agent0': np.ndarray((3,), dtype=float32, min=-0.281, max=0.981, mean=0.233)},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m   8: { 'agent0': np.ndarray((3,), dtype=float32, min=-1.462, max=0.0, mean=-0.831)},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m   9: { 'agent0': np.ndarray((3,), dtype=float32, min=-3.457, max=0.0, mean=-1.341)},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m   10: { 'agent0': np.ndarray((3,), dtype=float32, min=-1.626, max=0.8, mean=-0.275)},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m   11: { 'agent0': np.ndarray((3,), dtype=float32, min=0.0, max=1.64, mean=0.872)},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m   12: { 'agent0': np.ndarray((3,), dtype=float32, min=0.0, max=1.222, mean=0.408)},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m   13: { 'agent0': np.ndarray((3,), dtype=float32, min=-2.013, max=1.726, mean=-0.096)},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m   14: { 'agent0': np.ndarray((3,), dtype=float32, min=-0.819, max=0.075, mean=-0.248)},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m   15: { 'agent0': np.ndarray((3,), dtype=float32, min=0.0, max=0.902, mean=0.329)},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m   16: { 'agent0': np.ndarray((3,), dtype=float32, min=-1.604, max=0.0, mean=-1.024)},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m   17: { 'agent0': np.ndarray((3,), dtype=float32, min=-0.098, max=0.161, mean=0.021)},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m   18: { 'agent0': np.ndarray((3,), dtype=float32, min=-0.403, max=0.943, mean=0.18)},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m   19: { 'agent0': np.ndarray((3,), dtype=float32, min=-0.533, max=0.623, mean=0.03)}}\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m 2021-03-19 09:18:56,484\tINFO sampler.py:595 -- Info return from env: { 0: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m   1: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m   2: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m   3: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m   4: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m   5: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m   6: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m   7: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m   8: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m   9: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m   10: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m   11: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m   12: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m   13: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m   14: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m   15: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m   16: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m   17: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m   18: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m   19: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m 2021-03-19 09:18:56,485\tWARNING deprecation.py:33 -- DeprecationWarning: `env_index` has been deprecated. Use `episode.env_id` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m 2021-03-19 09:18:56,485\tINFO sampler.py:835 -- Preprocessed obs: np.ndarray((3,), dtype=float32, min=-0.93, max=1.511, mean=0.194)\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m 2021-03-19 09:18:56,486\tINFO sampler.py:840 -- Filtered obs: np.ndarray((3,), dtype=float32, min=-0.93, max=1.511, mean=0.194)\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m 2021-03-19 09:18:56,504\tINFO sampler.py:1249 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-0.93, max=1.511, mean=0.194),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'env_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-0.084, max=1.054, mean=0.323),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'env_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=0.0, max=0.553, mean=0.272),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'env_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-0.916, max=0.0, mean=-0.415),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'env_id': 4,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-0.551, max=0.647, mean=0.032),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'env_id': 5,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-1.421, max=1.746, mean=0.108),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'env_id': 6,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-1.815, max=0.0, mean=-0.934),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'env_id': 7,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-0.281, max=0.981, mean=0.233),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'env_id': 8,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-1.462, max=0.0, mean=-0.831),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'env_id': 9,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-3.457, max=0.0, mean=-1.341),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'env_id': 10,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-1.626, max=0.8, mean=-0.275),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'env_id': 11,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=0.0, max=1.64, mean=0.872),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'env_id': 12,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=0.0, max=1.222, mean=0.408),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'env_id': 13,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-2.013, max=1.726, mean=-0.096),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'env_id': 14,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-0.819, max=0.075, mean=-0.248),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'env_id': 15,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=0.0, max=0.902, mean=0.329),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'env_id': 16,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-1.604, max=0.0, mean=-1.024),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'env_id': 17,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-0.098, max=0.161, mean=0.021),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'env_id': 18,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-0.403, max=0.943, mean=0.18),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'env_id': 19,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-0.533, max=0.623, mean=0.03),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m 2021-03-19 09:18:56,535\tINFO sampler.py:1295 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m { 'default_policy': ( np.ndarray((20, 2), dtype=float32, min=-3.068, max=1.032, mean=-0.212),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                       {})}\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m 2021-03-19 09:18:56,591\tINFO rollout_worker.py:659 -- Generating sample batch of size 2000\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m 2021-03-19 09:18:56,600\tINFO sampler.py:593 -- Raw obs from env: { 0: { 'agent0': np.ndarray((3,), dtype=float32, min=-0.204, max=0.063, mean=-0.047)},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m   1: { 'agent0': np.ndarray((3,), dtype=float32, min=-1.109, max=0.803, mean=-0.102)},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m   2: { 'agent0': np.ndarray((3,), dtype=float32, min=0.0, max=0.998, mean=0.564)},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m   3: { 'agent0': np.ndarray((3,), dtype=float32, min=0.0, max=1.171, mean=0.561)},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m   4: { 'agent0': np.ndarray((3,), dtype=float32, min=-1.168, max=2.041, mean=0.291)},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m   5: { 'agent0': np.ndarray((3,), dtype=float32, min=-0.611, max=0.0, mean=-0.263)},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m   6: { 'agent0': np.ndarray((3,), dtype=float32, min=-0.291, max=0.204, mean=-0.029)},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m   7: { 'agent0': np.ndarray((3,), dtype=float32, min=0.0, max=1.509, mean=0.519)},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m   8: { 'agent0': np.ndarray((3,), dtype=float32, min=0.0, max=1.053, mean=0.392)},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m   9: { 'agent0': np.ndarray((3,), dtype=float32, min=-0.388, max=0.0, mean=-0.161)},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m   10: { 'agent0': np.ndarray((3,), dtype=float32, min=-0.39, max=0.0, mean=-0.191)},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m   11: { 'agent0': np.ndarray((3,), dtype=float32, min=-1.24, max=0.48, mean=-0.254)},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m   12: { 'agent0': np.ndarray((3,), dtype=float32, min=0.0, max=1.06, mean=0.612)},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m   13: { 'agent0': np.ndarray((3,), dtype=float32, min=-0.48, max=0.125, mean=-0.118)},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m   14: { 'agent0': np.ndarray((3,), dtype=float32, min=-1.439, max=0.84, mean=-0.2)},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m   15: { 'agent0': np.ndarray((3,), dtype=float32, min=-1.048, max=0.0, mean=-0.665)},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m   16: { 'agent0': np.ndarray((3,), dtype=float32, min=-1.472, max=1.189, mean=-0.094)},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m   17: { 'agent0': np.ndarray((3,), dtype=float32, min=-1.5, max=1.003, mean=-0.166)},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m   18: { 'agent0': np.ndarray((3,), dtype=float32, min=-1.55, max=0.0, mean=-0.691)},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m   19: { 'agent0': np.ndarray((3,), dtype=float32, min=-1.689, max=0.201, mean=-0.496)}}\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m 2021-03-19 09:18:56,601\tINFO sampler.py:595 -- Info return from env: { 0: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m   1: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m   2: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m   3: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m   4: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m   5: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m   6: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m   7: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m   8: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m   9: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m   10: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m   11: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m   12: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m   13: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m   14: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m   15: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m   16: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m   17: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m   18: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m   19: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m 2021-03-19 09:18:56,601\tWARNING deprecation.py:33 -- DeprecationWarning: `env_index` has been deprecated. Use `episode.env_id` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m 2021-03-19 09:18:56,602\tINFO sampler.py:835 -- Preprocessed obs: np.ndarray((3,), dtype=float32, min=-0.204, max=0.063, mean=-0.047)\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m 2021-03-19 09:18:56,602\tINFO sampler.py:840 -- Filtered obs: np.ndarray((3,), dtype=float32, min=-0.204, max=0.063, mean=-0.047)\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m 2021-03-19 09:18:56,619\tINFO sampler.py:1249 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-0.204, max=0.063, mean=-0.047),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'env_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-1.109, max=0.803, mean=-0.102),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'env_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=0.0, max=0.998, mean=0.564),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'env_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=0.0, max=1.171, mean=0.561),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'env_id': 4,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-1.168, max=2.041, mean=0.291),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'env_id': 5,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-0.611, max=0.0, mean=-0.263),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'env_id': 6,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-0.291, max=0.204, mean=-0.029),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'env_id': 7,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=0.0, max=1.509, mean=0.519),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'env_id': 8,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=0.0, max=1.053, mean=0.392),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'env_id': 9,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-0.388, max=0.0, mean=-0.161),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'env_id': 10,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-0.39, max=0.0, mean=-0.191),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'env_id': 11,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-1.24, max=0.48, mean=-0.254),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'env_id': 12,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=0.0, max=1.06, mean=0.612),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'env_id': 13,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-0.48, max=0.125, mean=-0.118),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'env_id': 14,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-1.439, max=0.84, mean=-0.2),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'env_id': 15,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-1.048, max=0.0, mean=-0.665),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'env_id': 16,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-1.472, max=1.189, mean=-0.094),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'env_id': 17,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-1.5, max=1.003, mean=-0.166),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'env_id': 18,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-1.55, max=0.0, mean=-0.691),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'env_id': 19,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-1.689, max=0.201, mean=-0.496),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m 2021-03-19 09:18:56,622\tINFO sampler.py:1295 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m { 'default_policy': ( np.ndarray((20, 2), dtype=float32, min=-2.959, max=2.315, mean=-0.231),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                       {})}\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m 2021-03-19 09:18:57,498\tINFO sample_batch_builder.py:209 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m { 'agent0': { 'data': { 'actions': np.ndarray((100, 2), dtype=float32, min=-10.123, max=10.187, mean=0.283),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                         'agent_index': np.ndarray((100,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                         'dones': np.ndarray((100,), dtype=bool, min=0.0, max=1.0, mean=0.01),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                         'eps_id': np.ndarray((100,), dtype=int64, min=757918461.0, max=757918461.0, mean=757918461.0),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                         'infos': np.ndarray((100,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                         'new_obs': np.ndarray((100, 3), dtype=float32, min=-12.026, max=100.0, mean=17.063),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                         'obs': np.ndarray((100, 3), dtype=float32, min=-12.026, max=99.0, mean=16.725),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                         'prev_actions': np.ndarray((100, 2), dtype=float32, min=-10.123, max=10.187, mean=0.282),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                         'prev_rewards': np.ndarray((100,), dtype=float32, min=-108.465, max=9.246, mean=-7.089),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                         'rewards': np.ndarray((100,), dtype=float32, min=-108.465, max=9.246, mean=-7.112),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                         't': np.ndarray((100,), dtype=int64, min=0.0, max=99.0, mean=49.5),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m                         'unroll_id': np.ndarray((100,), dtype=int64, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m 2021-03-19 09:18:57,515\tINFO sample_batch_builder.py:209 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m { 'agent0': { 'data': { 'actions': np.ndarray((100, 2), dtype=float32, min=-9.657, max=7.645, mean=0.018),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                         'agent_index': np.ndarray((100,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                         'dones': np.ndarray((100,), dtype=bool, min=0.0, max=1.0, mean=0.01),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                         'eps_id': np.ndarray((100,), dtype=int64, min=480708258.0, max=480708258.0, mean=480708258.0),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                         'infos': np.ndarray((100,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                         'new_obs': np.ndarray((100, 3), dtype=float32, min=-7.749, max=100.0, mean=16.852),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                         'obs': np.ndarray((100, 3), dtype=float32, min=-7.749, max=99.0, mean=16.522),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                         'prev_actions': np.ndarray((100, 2), dtype=float32, min=-9.657, max=7.645, mean=0.009),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                         'prev_rewards': np.ndarray((100,), dtype=float32, min=-40.291, max=8.398, mean=-4.256),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                         'rewards': np.ndarray((100,), dtype=float32, min=-40.291, max=8.398, mean=-4.25),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                         't': np.ndarray((100,), dtype=int64, min=0.0, max=99.0, mean=49.5),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m                         'unroll_id': np.ndarray((100,), dtype=int64, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb: Tracking run with wandb version 0.10.22\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb: Syncing run SVG(inf)\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/angelovtt/LQG-SVG\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb: 🚀 View run at https://wandb.ai/angelovtt/LQG-SVG/runs/1fo6gv70\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb: Run data is saved locally in /Users/angelolovatto/Repositories/personal/LQG-SVG/experiments/wandb/run-20210319_091853-1fo6gv70\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m 2021-03-19 09:18:57,582\tINFO sample_batch_builder.py:209 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m { 'agent0': { 'data': { 'actions': np.ndarray((100, 2), dtype=float32, min=-13.831, max=11.989, mean=-0.266),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                         'agent_index': np.ndarray((100,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                         'dones': np.ndarray((100,), dtype=bool, min=0.0, max=1.0, mean=0.01),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                         'eps_id': np.ndarray((100,), dtype=int64, min=1893281175.0, max=1893281175.0, mean=1893281175.0),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                         'infos': np.ndarray((100,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                         'new_obs': np.ndarray((100, 3), dtype=float32, min=-7.801, max=100.0, mean=16.893),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                         'obs': np.ndarray((100, 3), dtype=float32, min=-7.801, max=99.0, mean=16.544),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                         'prev_actions': np.ndarray((100, 2), dtype=float32, min=-13.831, max=11.989, mean=-0.23),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                         'prev_rewards': np.ndarray((100,), dtype=float32, min=-71.373, max=7.027, mean=-6.19),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                         'rewards': np.ndarray((100,), dtype=float32, min=-71.373, max=7.027, mean=-6.135),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                         't': np.ndarray((100,), dtype=int64, min=0.0, max=99.0, mean=49.5),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m                         'unroll_id': np.ndarray((100,), dtype=int64, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m 2021-03-19 09:18:57,610\tINFO rollout_worker.py:1114 -- Built policy map: {'default_policy': LQGPolicy(\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m   Box(-inf, inf, (3,), float32),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m   Box(-inf, inf, (2,), float32),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m   {\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m     compile: false\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m     env_config: {}\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m     exploration_config:\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m       pure_exploration_steps: 0\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m       type: raylab.utils.exploration.GaussianNoise\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m     explore: true\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m     framework: torch\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m     gamma: 0.99\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m     module: {}\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m     num_workers: 0\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m     optimizer: {}\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m     worker_index: 0\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m   }\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m )}\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m 2021-03-19 09:18:57,610\tINFO rollout_worker.py:1115 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x17c0e1fa0>}\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m 2021-03-19 09:18:57,610\tINFO rollout_worker.py:526 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x17c0e1f70>}\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m 2021-03-19 09:18:57,625\tINFO rollout_worker.py:697 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m { 'data': { 'actions': np.ndarray((2000, 2), dtype=float32, min=-33.417, max=27.353, mean=0.092),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m             'agent_index': np.ndarray((2000,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m             'dones': np.ndarray((2000,), dtype=bool, min=0.0, max=1.0, mean=0.01),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m             'eps_id': np.ndarray((2000,), dtype=int64, min=326378882.0, max=1960499143.0, mean=981409193.85),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m             'infos': np.ndarray((2000,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m             'new_obs': np.ndarray((2000, 3), dtype=float32, min=-29.617, max=100.0, mean=16.858),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m             'obs': np.ndarray((2000, 3), dtype=float32, min=-29.617, max=99.0, mean=16.519),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m             'prev_actions': np.ndarray((2000, 2), dtype=float32, min=-33.417, max=27.353, mean=0.088),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m             'prev_rewards': np.ndarray((2000,), dtype=float32, min=-581.447, max=18.176, mean=-9.322),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m             'rewards': np.ndarray((2000,), dtype=float32, min=-581.447, max=18.176, mean=-9.377),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m             't': np.ndarray((2000,), dtype=int64, min=0.0, max=99.0, mean=49.5),\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m             'unroll_id': np.ndarray((2000,), dtype=int64, min=0.0, max=19.0, mean=9.5)},\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m 2021-03-19 09:18:57,633\tINFO rollout_worker.py:697 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m { 'data': { 'actions': np.ndarray((2000, 2), dtype=float32, min=-14.88, max=17.203, mean=0.067),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m             'agent_index': np.ndarray((2000,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m             'dones': np.ndarray((2000,), dtype=bool, min=0.0, max=1.0, mean=0.01),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m             'eps_id': np.ndarray((2000,), dtype=int64, min=58445003.0, max=1888681515.0, mean=837622387.15),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m             'infos': np.ndarray((2000,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m             'new_obs': np.ndarray((2000, 3), dtype=float32, min=-16.566, max=100.0, mean=17.022),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m             'obs': np.ndarray((2000, 3), dtype=float32, min=-16.566, max=99.0, mean=16.682),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m             'prev_actions': np.ndarray((2000, 2), dtype=float32, min=-14.88, max=17.203, mean=0.059),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m             'prev_rewards': np.ndarray((2000,), dtype=float32, min=-198.305, max=13.669, mean=-5.257),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m             'rewards': np.ndarray((2000,), dtype=float32, min=-198.305, max=13.669, mean=-5.281),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m             't': np.ndarray((2000,), dtype=int64, min=0.0, max=99.0, mean=49.5),\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m             'unroll_id': np.ndarray((2000,), dtype=int64, min=0.0, max=19.0, mean=9.5)},\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m 2021-03-19 09:18:57,669\tINFO rollout_worker.py:659 -- Generating sample batch of size 2000\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m 2021-03-19 09:18:57,676\tINFO sampler.py:593 -- Raw obs from env: { 0: { 'agent0': np.ndarray((3,), dtype=float32, min=-1.283, max=0.0, mean=-0.592)},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m   1: { 'agent0': np.ndarray((3,), dtype=float32, min=-0.214, max=1.302, mean=0.363)},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m   2: { 'agent0': np.ndarray((3,), dtype=float32, min=0.0, max=1.303, mean=0.495)},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m   3: { 'agent0': np.ndarray((3,), dtype=float32, min=-1.207, max=0.0, mean=-0.411)},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m   4: { 'agent0': np.ndarray((3,), dtype=float32, min=0.0, max=2.273, mean=0.82)},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m   5: { 'agent0': np.ndarray((3,), dtype=float32, min=0.0, max=1.773, mean=0.992)},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m   6: { 'agent0': np.ndarray((3,), dtype=float32, min=-1.157, max=1.347, mean=0.063)},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m   7: { 'agent0': np.ndarray((3,), dtype=float32, min=-1.127, max=0.31, mean=-0.272)},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m   8: { 'agent0': np.ndarray((3,), dtype=float32, min=-0.854, max=0.0, mean=-0.38)},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m   9: { 'agent0': np.ndarray((3,), dtype=float32, min=-1.055, max=0.0, mean=-0.475)},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m   10: { 'agent0': np.ndarray((3,), dtype=float32, min=0.0, max=1.404, mean=0.631)},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m   11: { 'agent0': np.ndarray((3,), dtype=float32, min=-0.511, max=0.74, mean=0.076)},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m   12: { 'agent0': np.ndarray((3,), dtype=float32, min=-1.117, max=0.139, mean=-0.326)},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m   13: { 'agent0': np.ndarray((3,), dtype=float32, min=-0.968, max=0.604, mean=-0.121)},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m   14: { 'agent0': np.ndarray((3,), dtype=float32, min=-0.319, max=0.808, mean=0.163)},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m   15: { 'agent0': np.ndarray((3,), dtype=float32, min=-0.227, max=1.195, mean=0.322)},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m   16: { 'agent0': np.ndarray((3,), dtype=float32, min=-0.502, max=1.323, mean=0.274)},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m   17: { 'agent0': np.ndarray((3,), dtype=float32, min=-0.539, max=0.0, mean=-0.307)},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m   18: { 'agent0': np.ndarray((3,), dtype=float32, min=-0.291, max=0.978, mean=0.229)},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m   19: { 'agent0': np.ndarray((3,), dtype=float32, min=-1.643, max=0.0, mean=-0.911)}}\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m 2021-03-19 09:18:57,677\tINFO sampler.py:595 -- Info return from env: { 0: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m   1: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m   2: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m   3: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m   4: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m   5: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m   6: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m   7: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m   8: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m   9: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m   10: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m   11: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m   12: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m   13: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m   14: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m   15: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m   16: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m   17: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m   18: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m   19: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m 2021-03-19 09:18:57,677\tWARNING deprecation.py:33 -- DeprecationWarning: `env_index` has been deprecated. Use `episode.env_id` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m 2021-03-19 09:18:57,678\tINFO sampler.py:835 -- Preprocessed obs: np.ndarray((3,), dtype=float32, min=-1.283, max=0.0, mean=-0.592)\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m 2021-03-19 09:18:57,678\tINFO sampler.py:840 -- Filtered obs: np.ndarray((3,), dtype=float32, min=-1.283, max=0.0, mean=-0.592)\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m 2021-03-19 09:18:57,693\tINFO sampler.py:1249 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-1.283, max=0.0, mean=-0.592),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'env_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-0.214, max=1.302, mean=0.363),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'env_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=0.0, max=1.303, mean=0.495),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'env_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-1.207, max=0.0, mean=-0.411),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'env_id': 4,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=0.0, max=2.273, mean=0.82),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'env_id': 5,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=0.0, max=1.773, mean=0.992),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'env_id': 6,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-1.157, max=1.347, mean=0.063),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m 2021-03-19 09:18:57,678\tINFO rollout_worker.py:697 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m { 'data': { 'actions': np.ndarray((2000, 2), dtype=float32, min=-15.381, max=12.765, mean=-0.235),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m             'agent_index': np.ndarray((2000,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m             'dones': np.ndarray((2000,), dtype=bool, min=0.0, max=1.0, mean=0.01),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m             'eps_id': np.ndarray((2000,), dtype=int64, min=166159801.0, max=1893281175.0, mean=880535395.85),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m             'infos': np.ndarray((2000,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m             'new_obs': np.ndarray((2000, 3), dtype=float32, min=-15.138, max=100.0, mean=16.858),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m             'obs': np.ndarray((2000, 3), dtype=float32, min=-15.138, max=99.0, mean=16.512),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m             'prev_actions': np.ndarray((2000, 2), dtype=float32, min=-15.381, max=12.765, mean=-0.207),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m             'prev_rewards': np.ndarray((2000,), dtype=float32, min=-97.28, max=10.92, mean=-5.556),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m             'rewards': np.ndarray((2000,), dtype=float32, min=-97.28, max=10.92, mean=-5.529),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m             't': np.ndarray((2000,), dtype=int64, min=0.0, max=99.0, mean=49.5),\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m             'unroll_id': np.ndarray((2000,), dtype=int64, min=0.0, max=19.0, mean=9.5)},\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'env_id': 7,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-1.127, max=0.31, mean=-0.272),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'env_id': 8,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-0.854, max=0.0, mean=-0.38),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'env_id': 9,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-1.055, max=0.0, mean=-0.475),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'env_id': 10,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=0.0, max=1.404, mean=0.631),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'env_id': 11,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-0.511, max=0.74, mean=0.076),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'env_id': 12,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-1.117, max=0.139, mean=-0.326),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'env_id': 13,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-0.968, max=0.604, mean=-0.121),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'env_id': 14,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-0.319, max=0.808, mean=0.163),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'env_id': 15,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-0.227, max=1.195, mean=0.322),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'env_id': 16,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-0.502, max=1.323, mean=0.274),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'env_id': 17,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-0.539, max=0.0, mean=-0.307),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'env_id': 18,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-0.291, max=0.978, mean=0.229),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'env_id': 19,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'obs': np.ndarray((3,), dtype=float32, min=-1.643, max=0.0, mean=-0.911),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m 2021-03-19 09:18:57,697\tINFO sampler.py:1295 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m { 'default_policy': ( np.ndarray((20, 2), dtype=float32, min=-1.572, max=3.295, mean=0.696),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                       {})}\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m CalVer: 3.19.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m 2021-03-19 09:18:58,469\tINFO sample_batch_builder.py:209 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m { 'agent0': { 'data': { 'actions': np.ndarray((100, 2), dtype=float32, min=-6.2, max=7.0, mean=0.079),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                         'agent_index': np.ndarray((100,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                         'dones': np.ndarray((100,), dtype=bool, min=0.0, max=1.0, mean=0.01),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                         'eps_id': np.ndarray((100,), dtype=int64, min=652789120.0, max=652789120.0, mean=652789120.0),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                         'infos': np.ndarray((100,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                         'new_obs': np.ndarray((100, 3), dtype=float32, min=-7.543, max=100.0, mean=16.809),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                         'obs': np.ndarray((100, 3), dtype=float32, min=-7.543, max=99.0, mean=16.473),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                         'prev_actions': np.ndarray((100, 2), dtype=float32, min=-6.2, max=7.0, mean=0.069),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                         'prev_rewards': np.ndarray((100,), dtype=float32, min=-32.508, max=13.207, mean=-3.23),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                         'rewards': np.ndarray((100,), dtype=float32, min=-32.508, max=13.207, mean=-3.32),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                         't': np.ndarray((100,), dtype=int64, min=0.0, max=99.0, mean=49.5),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m                         'unroll_id': np.ndarray((100,), dtype=int64, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m 2021-03-19 09:18:58,545\tINFO rollout_worker.py:697 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m { 'data': { 'actions': np.ndarray((2000, 2), dtype=float32, min=-16.056, max=18.467, mean=0.068),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m             'agent_index': np.ndarray((2000,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m             'dones': np.ndarray((2000,), dtype=bool, min=0.0, max=1.0, mean=0.01),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m             'eps_id': np.ndarray((2000,), dtype=int64, min=96572050.0, max=1992317887.0, mean=1086866710.3),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m             'infos': np.ndarray((2000,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m             'new_obs': np.ndarray((2000, 3), dtype=float32, min=-15.424, max=100.0, mean=16.726),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m             'obs': np.ndarray((2000, 3), dtype=float32, min=-15.424, max=99.0, mean=16.392),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m             'prev_actions': np.ndarray((2000, 2), dtype=float32, min=-16.056, max=18.467, mean=0.073),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m             'prev_rewards': np.ndarray((2000,), dtype=float32, min=-83.034, max=14.41, mean=-4.749),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m             'rewards': np.ndarray((2000,), dtype=float32, min=-83.034, max=14.41, mean=-4.759),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m             't': np.ndarray((2000,), dtype=int64, min=0.0, max=99.0, mean=49.5),\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m             'unroll_id': np.ndarray((2000,), dtype=int64, min=0.0, max=19.0, mean=9.5)},\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb: Waiting for W&B process to finish, PID 3034\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb: Program ended successfully.\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: Waiting for W&B process to finish, PID 3033\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: Program ended successfully.\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb: - 0.77MB of 0.77MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb: Waiting for W&B process to finish, PID 3032\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb: Program ended successfully.\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 0.66MB of 0.78MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb: \\ 1.21MB of 1.23MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb: - 0.77MB of 0.77MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.12MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb: | 1.18MB of 1.23MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb: \\ 0.65MB of 1.23MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.06MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb: / 1.16MB of 1.23MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb: | 0.94MB of 1.23MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.08MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb: - 1.17MB of 1.23MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb: / 1.04MB of 1.23MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.08MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb: \\ 1.19MB of 1.23MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb: - 1.13MB of 1.23MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.10MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb: | 1.19MB of 1.23MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.11MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb: \\ 1.13MB of 1.23MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb: / 1.21MB of 1.23MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.11MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb: | 1.15MB of 1.23MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb: - 1.22MB of 1.23MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb: Waiting for W&B process to finish, PID 3031\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb: Program ended successfully.\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb: / 1.16MB of 1.23MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb: \\ 1.23MB of 1.23MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.14MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb: - 1.17MB of 1.23MB uploaded (0.00MB deduped)\n",
      "wandb:                                                                                \n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.15MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb: \\ 1.21MB of 1.23MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb: - 0.78MB of 0.78MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb: Find user logs for this run at: /Users/angelolovatto/Repositories/personal/LQG-SVG/experiments/wandb/run-20210319_091853-3dz43gtk/logs/debug.log\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb: Find internal logs for this run at: /Users/angelolovatto/Repositories/personal/LQG-SVG/experiments/wandb/run-20210319_091853-3dz43gtk/logs/debug-internal.log\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb:          model_epochs 4\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb:             model_nll 421.46664\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb:             iteration 199\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb:   distance_to_optimal 3.42307\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb:    episode_reward_max -270.23819\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb:    episode_reward_min -818.69785\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb:   episode_reward_mean -513.69663\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb:      episode_len_mean 100.0\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb:    episodes_this_iter 20\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb:              _runtime 338\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb:            _timestamp 1616156671\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb:                 _step 199\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb:          model_epochs █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb:             model_nll ▁▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb:             iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb:   distance_to_optimal ██▇▇▇▇▆▆▆▆▆▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb:    episode_reward_max ▁▅▄▂█▇▂▅▄▃▄▄▄▄▄▄▆▄▄█▆▄▃▄▃▅▅▅▃▅▆▅▅▄▄▅▄▃▅▅\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb:    episode_reward_min ▄▄▃▂▃▅▅▅▆▄▇▅▄▅▁▂▁▆▆▆▄▅▅▂▄▃▆██▆▅▇▅▇█▅██▆█\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb:   episode_reward_mean ▄▂▃▄▁▂▁▄▅▃▃▄▄▅▄▅▅▅▆▅▄▅▅▅▆▄▆█▄▆▆▅▆▇▆▇▆▆▅▇\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb:      episode_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb:    episodes_this_iter ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb:              _runtime ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb:            _timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb: Synced 6 W&B file(s), 0 media file(s), 22 artifact file(s) and 422 other file(s)\n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(pid=3002)\u001b[0m wandb: Synced SVG(inf): https://wandb.ai/angelovtt/LQG-SVG/runs/3dz43gtk\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.15MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb: | 1.23MB of 1.23MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.15MB of 1.24MB uploaded (0.00MB deduped)\n",
      "wandb:                                                                                \n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb: \\ 1.24MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.15MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb: Find user logs for this run at: /Users/angelolovatto/Repositories/personal/LQG-SVG/experiments/wandb/run-20210319_091853-2y5fpjmo/logs/debug.log\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb: Find internal logs for this run at: /Users/angelolovatto/Repositories/personal/LQG-SVG/experiments/wandb/run-20210319_091853-2y5fpjmo/logs/debug-internal.log\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb:          model_epochs 5\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb:             model_nll 444.53702\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb:             iteration 199\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb:   distance_to_optimal 3.183\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb:    episode_reward_max -190.67614\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb:    episode_reward_min -697.49022\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb:   episode_reward_mean -406.29924\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb:      episode_len_mean 100.0\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb:    episodes_this_iter 20\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb:              _runtime 341\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb:            _timestamp 1616156674\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb:                 _step 199\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb:          model_epochs ▁██▁▁█▁▁▁▁▁▁████▁▁▁██▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb:             model_nll ▂▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb:             iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb:   distance_to_optimal ██▇▇▇▇▆▆▆▆▆▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb:    episode_reward_max ▁▃▃▃▆▂▆▆▃▄▃▅▂▁▄▆▆▅▄▇▇▆▆▅▂▆▃▄▅▄▆▅▃▄▅▂██▂▇\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb:    episode_reward_min ▅▄▄▆▄▄▄▄▆▆▃▁▇▆▅▆▆▅▆▄▄▆▆▆▆▇▅▇▅▅██▅▅▆▅█▇█▇\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb:   episode_reward_mean ▁▂▃▂▃▁▂▂▃▃▄▃▄▄▄▄▆▇▆▅▆▆▆▅▅▇▅▆▅▇▇▆▆▅▆▅██▇█\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb:      episode_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb:    episodes_this_iter ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb:              _runtime ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb:            _timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb: Synced 6 W&B file(s), 0 media file(s), 22 artifact file(s) and 422 other file(s)\n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(pid=3000)\u001b[0m wandb: Synced SVG(inf): https://wandb.ai/angelovtt/LQG-SVG/runs/2y5fpjmo\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb: | 1.24MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.15MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb: / 1.23MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.15MB of 1.24MB uploaded (0.00MB deduped)\n",
      "wandb:                                                                                \n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.15MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb: Find user logs for this run at: /Users/angelolovatto/Repositories/personal/LQG-SVG/experiments/wandb/run-20210319_091853-1fo6gv70/logs/debug.log\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb: Find internal logs for this run at: /Users/angelolovatto/Repositories/personal/LQG-SVG/experiments/wandb/run-20210319_091853-1fo6gv70/logs/debug-internal.log\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb:          model_epochs 4\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb:             model_nll 408.07858\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb:             iteration 199\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb:   distance_to_optimal 3.46509\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb:    episode_reward_max -234.0605\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb:    episode_reward_min -853.36704\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb:   episode_reward_mean -468.49133\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb:      episode_len_mean 100.0\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb:    episodes_this_iter 20\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb:              _runtime 360\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb:            _timestamp 1616156693\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb:                 _step 199\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb:          model_epochs █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb:             model_nll ▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇███████\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb:             iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb:   distance_to_optimal ██▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb:    episode_reward_max ▃▃▂▁▁▁▃▃▂▃▄▃▅▅▄▆▆▄▄▄▄▂▆▃▆▅▃▄▄▃▆▆▅█▇▆▅▅█▄\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb:    episode_reward_min ▅▂▅▂▄▄▆▄▅▄▇▁▅▆▅▄▆▂▆▆▇▆▆▂▇█▂▂▆▅▁▆▆█▆█▆▆▇▇\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb:   episode_reward_mean ▄▁▃▂▂▂▃▃▂▄▄▄▄▄▃▄▆▄▄▄▅▃▅▅▅▆▄▆▅▆▆▆▆▇█▆▆▆▇▇\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb:      episode_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb:    episodes_this_iter ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇█\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇█\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb: Synced 6 W&B file(s), 0 media file(s), 22 artifact file(s) and 422 other file(s)\n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(pid=3001)\u001b[0m wandb: Synced SVG(inf): https://wandb.ai/angelovtt/LQG-SVG/runs/1fo6gv70\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.15MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.15MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.15MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.15MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.15MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.15MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.15MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.15MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.15MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.15MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.15MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.15MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.15MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.15MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.15MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.15MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.15MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.15MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.15MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.15MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.15MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.15MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.15MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.15MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.15MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.15MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.15MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.15MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.15MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.15MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.15MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.15MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.15MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.18MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.18MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.18MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.18MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.18MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.18MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.18MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.18MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.18MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.18MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.18MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.18MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.18MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.18MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.18MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.18MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.18MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.18MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.18MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: Network error (TransientException), entering retry loop. See wandb/debug-internal.log for full traceback.\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: - 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: \\ 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: | 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: / 1.13MB of 1.24MB uploaded (0.00MB deduped)\n",
      "\u001b[2m\u001b[36m(pid=3003)\u001b[0m wandb: ERROR Control-C detected -- Run data was not synced\n",
      "2021-03-19 09:36:55,460\tWARNING worker.py:1107 -- A worker died or was killed while executing task ffffffffffffffff69a6825d641b461327313d1c01000000.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e8e323a2c3a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msmall_exps\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/lqsvg-kEsPsxvA-py3.8/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclient_mode_enabled\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_client_hook_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/lqsvg-kEsPsxvA-py3.8/lib/python3.8/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   1446\u001b[0m         \u001b[0;32mglobal\u001b[0m \u001b[0mlast_task_error_raise_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \u001b[0;31m# TODO(ujvl): Consider how to allow user to retrieve the ready objects.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m         values, debugger_breakpoint = worker.get_objects(\n\u001b[0m\u001b[1;32m   1449\u001b[0m             object_refs, timeout=timeout)\n\u001b[1;32m   1450\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/lqsvg-kEsPsxvA-py3.8/lib/python3.8/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mget_objects\u001b[0;34m(self, object_refs, timeout)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0mtimeout_ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m         data_metadata_pairs = self.core_worker.get_objects(\n\u001b[0m\u001b[1;32m    310\u001b[0m             object_refs, self.current_task_id, timeout_ms)\n\u001b[1;32m    311\u001b[0m         \u001b[0mdebugger_breakpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ray.init(\n",
    "    logging_level=logging.WARNING\n",
    ")\n",
    "\n",
    "small_exps = [\n",
    "    Experiment.remote(\n",
    "        {\n",
    "            \"iterations\": 200,\n",
    "            \"trajs_per_iter\": 20,\n",
    "            \"policy_lr\": 3e-4,\n",
    "            \"improvement_delta\": 0.0,\n",
    "            \"patience\": 3,\n",
    "            \"svg_samples\": 32,\n",
    "            \"dataset_batch_size\": 32,\n",
    "            \"train_val_split\": (0.8, 0.2),\n",
    "            \"model_lr\": 1e-3,\n",
    "            \"env_config\": dict(n_state=2, n_ctrl=2, horizon=100, num_envs=20),\n",
    "            \"true_model\": False,\n",
    "        }\n",
    "    )\n",
    "    for _ in range(4)\n",
    "]\n",
    "for exp in small_exps:\n",
    "    exp.setup.remote()\n",
    "    \n",
    "ray.get([e.execute.remote() for e in small_exps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "completed-level",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m CalVer: 3.19.0\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m CalVer: 3.19.0\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m CalVer: 3.19.0\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m CalVer: 3.19.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m wandb: W&B syncing is set to `offline` in this directory.  Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m wandb: W&B syncing is set to `offline` in this directory.  Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m wandb: W&B syncing is set to `offline` in this directory.  Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m wandb: W&B syncing is set to `offline` in this directory.  Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m 2021-03-19 09:37:48,548\tINFO rollout_worker.py:1114 -- Built policy map: {'default_policy': LQGPolicy(\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m   Box(-inf, inf, (9,), float32),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m   Box(-inf, inf, (8,), float32),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m   {\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m     compile: false\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m     env_config: {}\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m     exploration_config:\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m       pure_exploration_steps: 0\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m       type: raylab.utils.exploration.GaussianNoise\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m     explore: true\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m     framework: torch\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m     gamma: 0.99\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m     module: {}\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m     num_workers: 0\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m     optimizer: {}\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m     worker_index: 0\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m   }\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m )}\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m 2021-03-19 09:37:48,548\tINFO rollout_worker.py:1115 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x178516e80>}\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m 2021-03-19 09:37:48,548\tINFO rollout_worker.py:526 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x178516e50>}\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m 2021-03-19 09:37:48,564\tINFO rollout_worker.py:1114 -- Built policy map: {'default_policy': LQGPolicy(\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m   Box(-inf, inf, (9,), float32),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m   Box(-inf, inf, (8,), float32),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m   {\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m     compile: false\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m     env_config: {}\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m     exploration_config:\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m       pure_exploration_steps: 0\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m       type: raylab.utils.exploration.GaussianNoise\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m     explore: true\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m     framework: torch\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m     gamma: 0.99\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m     module: {}\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m     num_workers: 0\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m     optimizer: {}\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m     worker_index: 0\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m   }\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m )}\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m 2021-03-19 09:37:48,564\tINFO rollout_worker.py:1115 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x177cede80>}\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m 2021-03-19 09:37:48,565\tINFO rollout_worker.py:526 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x177cede50>}\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m 2021-03-19 09:37:48,545\tINFO rollout_worker.py:1114 -- Built policy map: {'default_policy': LQGPolicy(\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m   Box(-inf, inf, (9,), float32),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m   Box(-inf, inf, (8,), float32),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m   {\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m     compile: false\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m     env_config: {}\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m     exploration_config:\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m       pure_exploration_steps: 0\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m       type: raylab.utils.exploration.GaussianNoise\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m     explore: true\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m     framework: torch\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m     gamma: 0.99\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m     module: {}\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m     num_workers: 0\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m     optimizer: {}\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m     worker_index: 0\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m   }\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m )}\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m 2021-03-19 09:37:48,546\tINFO rollout_worker.py:1115 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x184a1ae80>}\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m 2021-03-19 09:37:48,546\tINFO rollout_worker.py:526 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x184a1ae50>}\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m 2021-03-19 09:37:48,597\tINFO rollout_worker.py:1114 -- Built policy map: {'default_policy': LQGPolicy(\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m   Box(-inf, inf, (9,), float32),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m   Box(-inf, inf, (8,), float32),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m   {\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m     compile: false\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m     env_config: {}\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m     exploration_config:\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m       pure_exploration_steps: 0\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m       type: raylab.utils.exploration.GaussianNoise\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m     explore: true\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m     framework: torch\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m     gamma: 0.99\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m     module: {}\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m     num_workers: 0\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m     optimizer: {}\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m     worker_index: 0\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m   }\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m )}\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m 2021-03-19 09:37:48,597\tINFO rollout_worker.py:1115 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x184989e80>}\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m 2021-03-19 09:37:48,598\tINFO rollout_worker.py:526 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x184989e50>}\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m 2021-03-19 09:37:49,173\tINFO rollout_worker.py:659 -- Generating sample batch of size 20000\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m 2021-03-19 09:37:49,135\tINFO rollout_worker.py:659 -- Generating sample batch of size 20000\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m 2021-03-19 09:37:49,145\tINFO sampler.py:593 -- Raw obs from env: { 0: { 'agent0': np.ndarray((9,), dtype=float32, min=-1.201, max=0.982, mean=-0.165)},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m   1: { 'agent0': np.ndarray((9,), dtype=float32, min=-1.369, max=1.869, mean=0.02)},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m   2: { 'agent0': np.ndarray((9,), dtype=float32, min=-3.461, max=0.993, mean=-0.582)},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m   3: { 'agent0': np.ndarray((9,), dtype=float32, min=-2.051, max=0.136, mean=-0.629)},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m   4: { 'agent0': np.ndarray((9,), dtype=float32, min=-0.684, max=0.95, mean=0.201)},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m   5: { 'agent0': np.ndarray((9,), dtype=float32, min=-1.387, max=0.937, mean=-0.308)},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m   6: { 'agent0': np.ndarray((9,), dtype=float32, min=-0.725, max=1.06, mean=-0.139)},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m   7: { 'agent0': np.ndarray((9,), dtype=float32, min=-0.592, max=2.799, mean=0.633)},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m   8: { 'agent0': np.ndarray((9,), dtype=float32, min=-0.999, max=0.705, mean=0.004)},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m   9: { 'agent0': np.ndarray((9,), dtype=float32, min=-0.66, max=2.272, mean=0.166)},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m   10: { 'agent0': np.ndarray((9,), dtype=float32, min=-1.931, max=0.717, mean=-0.488)},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m   11: { 'agent0': np.ndarray((9,), dtype=float32, min=-1.547, max=1.053, mean=-0.112)},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m   12: { 'agent0': np.ndarray((9,), dtype=float32, min=-2.435, max=1.401, mean=-0.358)},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m   13: { 'agent0': np.ndarray((9,), dtype=float32, min=-1.87, max=0.961, mean=-0.579)},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m   14: { 'agent0': np.ndarray((9,), dtype=float32, min=-0.428, max=1.697, mean=0.245)},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m   15: { 'agent0': np.ndarray((9,), dtype=float32, min=-1.284, max=1.314, mean=-0.003)},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m   16: { 'agent0': np.ndarray((9,), dtype=float32, min=-1.731, max=0.864, mean=-0.332)},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m   17: { 'agent0': np.ndarray((9,), dtype=float32, min=-0.946, max=1.625, mean=-0.005)},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m   18: { 'agent0': np.ndarray((9,), dtype=float32, min=-1.148, max=1.267, mean=-0.247)},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m   19: { 'agent0': np.ndarray((9,), dtype=float32, min=-2.161, max=1.455, mean=-0.34)}}\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m 2021-03-19 09:37:49,146\tINFO sampler.py:595 -- Info return from env: { 0: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m   1: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m   2: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m   3: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m   4: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m   5: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m   6: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m   7: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m   8: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m   9: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m   10: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m   11: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m   12: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m   13: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m   14: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m   15: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m   16: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m   17: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m   18: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m   19: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m 2021-03-19 09:37:49,147\tWARNING deprecation.py:33 -- DeprecationWarning: `env_index` has been deprecated. Use `episode.env_id` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m 2021-03-19 09:37:49,147\tINFO sampler.py:835 -- Preprocessed obs: np.ndarray((9,), dtype=float32, min=-1.201, max=0.982, mean=-0.165)\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m 2021-03-19 09:37:49,147\tINFO sampler.py:840 -- Filtered obs: np.ndarray((9,), dtype=float32, min=-1.201, max=0.982, mean=-0.165)\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m 2021-03-19 09:37:49,166\tINFO sampler.py:1249 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-1.201, max=0.982, mean=-0.165),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'env_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-1.369, max=1.869, mean=0.02),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'env_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-3.461, max=0.993, mean=-0.582),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'env_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-2.051, max=0.136, mean=-0.629),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'env_id': 4,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-0.684, max=0.95, mean=0.201),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'env_id': 5,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-1.387, max=0.937, mean=-0.308),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'env_id': 6,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-0.725, max=1.06, mean=-0.139),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m 2021-03-19 09:37:49,138\tINFO rollout_worker.py:659 -- Generating sample batch of size 20000\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m 2021-03-19 09:37:49,147\tINFO sampler.py:593 -- Raw obs from env: { 0: { 'agent0': np.ndarray((9,), dtype=float32, min=-1.145, max=1.284, mean=0.136)},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m   1: { 'agent0': np.ndarray((9,), dtype=float32, min=-2.07, max=0.731, mean=-0.352)},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m   2: { 'agent0': np.ndarray((9,), dtype=float32, min=-0.671, max=2.065, mean=0.622)},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m   3: { 'agent0': np.ndarray((9,), dtype=float32, min=-1.095, max=2.244, mean=0.766)},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m   4: { 'agent0': np.ndarray((9,), dtype=float32, min=-1.527, max=1.433, mean=0.089)},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m   5: { 'agent0': np.ndarray((9,), dtype=float32, min=-2.133, max=1.386, mean=-0.437)},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m   6: { 'agent0': np.ndarray((9,), dtype=float32, min=-2.652, max=1.191, mean=0.115)},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m   7: { 'agent0': np.ndarray((9,), dtype=float32, min=-1.641, max=1.075, mean=0.17)},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m   8: { 'agent0': np.ndarray((9,), dtype=float32, min=-1.665, max=1.315, mean=-0.103)},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m   9: { 'agent0': np.ndarray((9,), dtype=float32, min=-1.875, max=1.737, mean=0.349)},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m   10: { 'agent0': np.ndarray((9,), dtype=float32, min=-0.98, max=1.641, mean=0.435)},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m   11: { 'agent0': np.ndarray((9,), dtype=float32, min=-2.736, max=1.21, mean=-0.506)},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m   12: { 'agent0': np.ndarray((9,), dtype=float32, min=-1.749, max=3.586, mean=0.404)},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m   13: { 'agent0': np.ndarray((9,), dtype=float32, min=-1.458, max=1.52, mean=-0.132)},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m   14: { 'agent0': np.ndarray((9,), dtype=float32, min=-1.362, max=0.975, mean=-0.04)},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m   15: { 'agent0': np.ndarray((9,), dtype=float32, min=-1.542, max=1.506, mean=0.158)},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m   16: { 'agent0': np.ndarray((9,), dtype=float32, min=-1.417, max=1.092, mean=-0.111)},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m   17: { 'agent0': np.ndarray((9,), dtype=float32, min=-0.751, max=1.388, mean=0.387)},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m   18: { 'agent0': np.ndarray((9,), dtype=float32, min=-1.089, max=1.477, mean=-0.296)},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m   19: { 'agent0': np.ndarray((9,), dtype=float32, min=-0.191, max=1.338, mean=0.591)}}\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m 2021-03-19 09:37:49,148\tINFO sampler.py:595 -- Info return from env: { 0: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m   1: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m   2: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m   3: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m   4: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m   5: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m   6: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m   7: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m   8: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m   9: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m   10: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m   11: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m   12: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m   13: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m   14: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m   15: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m   16: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m   17: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m   18: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m   19: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m 2021-03-19 09:37:49,148\tWARNING deprecation.py:33 -- DeprecationWarning: `env_index` has been deprecated. Use `episode.env_id` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m 2021-03-19 09:37:49,148\tINFO sampler.py:835 -- Preprocessed obs: np.ndarray((9,), dtype=float32, min=-1.145, max=1.284, mean=0.136)\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m 2021-03-19 09:37:49,149\tINFO sampler.py:840 -- Filtered obs: np.ndarray((9,), dtype=float32, min=-1.145, max=1.284, mean=0.136)\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m 2021-03-19 09:37:49,168\tINFO sampler.py:1249 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-1.145, max=1.284, mean=0.136),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'env_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-2.07, max=0.731, mean=-0.352),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'env_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-0.671, max=2.065, mean=0.622),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'env_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-1.095, max=2.244, mean=0.766),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'env_id': 4,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-1.527, max=1.433, mean=0.089),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'env_id': 5,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-2.133, max=1.386, mean=-0.437),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'env_id': 6,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-2.652, max=1.191, mean=0.115),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m 2021-03-19 09:37:49,131\tINFO rollout_worker.py:659 -- Generating sample batch of size 20000\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m 2021-03-19 09:37:49,140\tINFO sampler.py:593 -- Raw obs from env: { 0: { 'agent0': np.ndarray((9,), dtype=float32, min=-1.488, max=1.137, mean=-0.344)},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m   1: { 'agent0': np.ndarray((9,), dtype=float32, min=-0.98, max=1.74, mean=0.177)},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m   2: { 'agent0': np.ndarray((9,), dtype=float32, min=-0.954, max=1.105, mean=0.217)},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m   3: { 'agent0': np.ndarray((9,), dtype=float32, min=-1.939, max=1.768, mean=0.606)},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m   4: { 'agent0': np.ndarray((9,), dtype=float32, min=-1.287, max=1.796, mean=0.239)},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m   5: { 'agent0': np.ndarray((9,), dtype=float32, min=-0.747, max=0.929, mean=0.194)},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m   6: { 'agent0': np.ndarray((9,), dtype=float32, min=-1.404, max=1.43, mean=0.065)},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m   7: { 'agent0': np.ndarray((9,), dtype=float32, min=-1.565, max=1.944, mean=-0.085)},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m   8: { 'agent0': np.ndarray((9,), dtype=float32, min=-1.15, max=1.906, mean=-0.146)},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m   9: { 'agent0': np.ndarray((9,), dtype=float32, min=-0.318, max=1.44, mean=0.336)},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m   10: { 'agent0': np.ndarray((9,), dtype=float32, min=-2.022, max=1.139, mean=0.08)},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m   11: { 'agent0': np.ndarray((9,), dtype=float32, min=-0.981, max=1.535, mean=0.066)},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m   12: { 'agent0': np.ndarray((9,), dtype=float32, min=-1.109, max=1.179, mean=-0.066)},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m   13: { 'agent0': np.ndarray((9,), dtype=float32, min=-0.988, max=2.026, mean=0.408)},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m   14: { 'agent0': np.ndarray((9,), dtype=float32, min=-0.504, max=1.723, mean=0.41)},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m   15: { 'agent0': np.ndarray((9,), dtype=float32, min=-2.618, max=1.178, mean=-0.09)},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m   16: { 'agent0': np.ndarray((9,), dtype=float32, min=-1.239, max=2.099, mean=0.518)},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m   17: { 'agent0': np.ndarray((9,), dtype=float32, min=-2.033, max=1.126, mean=-0.026)},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m   18: { 'agent0': np.ndarray((9,), dtype=float32, min=-2.218, max=1.493, mean=-0.013)},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m   19: { 'agent0': np.ndarray((9,), dtype=float32, min=-1.656, max=2.455, mean=0.08)}}\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m 2021-03-19 09:37:49,141\tINFO sampler.py:595 -- Info return from env: { 0: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m   1: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m   2: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m   3: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m   4: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m   5: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m   6: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m   7: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m   8: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m   9: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m   10: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m   11: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m   12: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m   13: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m   14: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m   15: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m   16: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m   17: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m   18: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m   19: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m 2021-03-19 09:37:49,142\tWARNING deprecation.py:33 -- DeprecationWarning: `env_index` has been deprecated. Use `episode.env_id` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m 2021-03-19 09:37:49,142\tINFO sampler.py:835 -- Preprocessed obs: np.ndarray((9,), dtype=float32, min=-1.488, max=1.137, mean=-0.344)\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m 2021-03-19 09:37:49,142\tINFO sampler.py:840 -- Filtered obs: np.ndarray((9,), dtype=float32, min=-1.488, max=1.137, mean=-0.344)\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m 2021-03-19 09:37:49,159\tINFO sampler.py:1249 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-1.488, max=1.137, mean=-0.344),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'env_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-0.98, max=1.74, mean=0.177),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'env_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-0.954, max=1.105, mean=0.217),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'env_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-1.939, max=1.768, mean=0.606),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'env_id': 4,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-1.287, max=1.796, mean=0.239),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'env_id': 5,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-0.747, max=0.929, mean=0.194),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'env_id': 6,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-1.404, max=1.43, mean=0.065),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'env_id': 7,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-0.592, max=2.799, mean=0.633),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'env_id': 8,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-0.999, max=0.705, mean=0.004),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'env_id': 9,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-0.66, max=2.272, mean=0.166),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'env_id': 10,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-1.931, max=0.717, mean=-0.488),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'env_id': 11,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-1.547, max=1.053, mean=-0.112),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'env_id': 12,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-2.435, max=1.401, mean=-0.358),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'env_id': 13,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-1.87, max=0.961, mean=-0.579),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'env_id': 14,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-0.428, max=1.697, mean=0.245),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'env_id': 15,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-1.284, max=1.314, mean=-0.003),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'env_id': 16,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-1.731, max=0.864, mean=-0.332),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'env_id': 17,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-0.946, max=1.625, mean=-0.005),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'env_id': 18,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-1.148, max=1.267, mean=-0.247),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'env_id': 19,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'env_id': 7,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-1.641, max=1.075, mean=0.17),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'env_id': 8,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-1.665, max=1.315, mean=-0.103),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'env_id': 9,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-1.875, max=1.737, mean=0.349),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'env_id': 10,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-0.98, max=1.641, mean=0.435),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'env_id': 11,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-2.736, max=1.21, mean=-0.506),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'env_id': 12,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-1.749, max=3.586, mean=0.404),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'env_id': 13,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-1.458, max=1.52, mean=-0.132),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'env_id': 14,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-1.362, max=0.975, mean=-0.04),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'env_id': 15,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-1.542, max=1.506, mean=0.158),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'env_id': 16,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-1.417, max=1.092, mean=-0.111),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'env_id': 17,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-0.751, max=1.388, mean=0.387),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'env_id': 18,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-1.089, max=1.477, mean=-0.296),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'env_id': 19,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'env_id': 7,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-1.565, max=1.944, mean=-0.085),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'env_id': 8,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-1.15, max=1.906, mean=-0.146),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'env_id': 9,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-0.318, max=1.44, mean=0.336),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'env_id': 10,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-2.022, max=1.139, mean=0.08),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'env_id': 11,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-0.981, max=1.535, mean=0.066),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'env_id': 12,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-1.109, max=1.179, mean=-0.066),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'env_id': 13,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-0.988, max=2.026, mean=0.408),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'env_id': 14,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-0.504, max=1.723, mean=0.41),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'env_id': 15,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-2.618, max=1.178, mean=-0.09),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'env_id': 16,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-1.239, max=2.099, mean=0.518),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'env_id': 17,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-2.033, max=1.126, mean=-0.026),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'env_id': 18,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-2.218, max=1.493, mean=-0.013),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'env_id': 19,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-2.161, max=1.455, mean=-0.34),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m 2021-03-19 09:37:49,169\tINFO sampler.py:1295 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m { 'default_policy': ( np.ndarray((20, 8), dtype=float32, min=-7.282, max=5.254, mean=0.285),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                       {})}\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-0.191, max=1.338, mean=0.591),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m 2021-03-19 09:37:49,172\tINFO sampler.py:1295 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m { 'default_policy': ( np.ndarray((20, 8), dtype=float32, min=-4.427, max=2.481, mean=-0.206),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                       {})}\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-1.656, max=2.455, mean=0.08),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m 2021-03-19 09:37:49,162\tINFO sampler.py:1295 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m { 'default_policy': ( np.ndarray((20, 8), dtype=float32, min=-7.098, max=4.033, mean=-0.144),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                       {})}\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m 2021-03-19 09:37:49,182\tINFO sampler.py:593 -- Raw obs from env: { 0: { 'agent0': np.ndarray((9,), dtype=float32, min=-1.276, max=1.776, mean=-0.104)},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m   1: { 'agent0': np.ndarray((9,), dtype=float32, min=-0.672, max=1.015, mean=-0.192)},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m   2: { 'agent0': np.ndarray((9,), dtype=float32, min=-2.427, max=0.29, mean=-0.419)},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m   3: { 'agent0': np.ndarray((9,), dtype=float32, min=-1.26, max=1.364, mean=-0.052)},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m   4: { 'agent0': np.ndarray((9,), dtype=float32, min=-0.653, max=1.716, mean=0.241)},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m   5: { 'agent0': np.ndarray((9,), dtype=float32, min=-2.25, max=1.028, mean=-0.436)},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m   6: { 'agent0': np.ndarray((9,), dtype=float32, min=-2.174, max=2.271, mean=-0.178)},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m   7: { 'agent0': np.ndarray((9,), dtype=float32, min=-0.729, max=0.847, mean=0.205)},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m   8: { 'agent0': np.ndarray((9,), dtype=float32, min=-1.384, max=3.104, mean=-0.097)},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m   9: { 'agent0': np.ndarray((9,), dtype=float32, min=-0.166, max=1.843, mean=0.8)},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m   10: { 'agent0': np.ndarray((9,), dtype=float32, min=-1.744, max=0.6, mean=-0.401)},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m   11: { 'agent0': np.ndarray((9,), dtype=float32, min=-1.697, max=0.745, mean=-0.188)},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m   12: { 'agent0': np.ndarray((9,), dtype=float32, min=-0.878, max=0.23, mean=-0.293)},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m   13: { 'agent0': np.ndarray((9,), dtype=float32, min=-1.142, max=0.366, mean=-0.358)},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m   14: { 'agent0': np.ndarray((9,), dtype=float32, min=-1.457, max=1.358, mean=0.01)},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m   15: { 'agent0': np.ndarray((9,), dtype=float32, min=-1.91, max=1.405, mean=0.043)},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m   16: { 'agent0': np.ndarray((9,), dtype=float32, min=-1.907, max=1.191, mean=-0.158)},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m   17: { 'agent0': np.ndarray((9,), dtype=float32, min=-1.303, max=1.527, mean=0.085)},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m   18: { 'agent0': np.ndarray((9,), dtype=float32, min=-2.183, max=1.399, mean=-0.004)},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m   19: { 'agent0': np.ndarray((9,), dtype=float32, min=-0.334, max=1.032, mean=0.339)}}\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m 2021-03-19 09:37:49,183\tINFO sampler.py:595 -- Info return from env: { 0: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m   1: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m   2: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m   3: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m   4: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m   5: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m   6: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m   7: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m   8: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m   9: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m   10: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m   11: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m   12: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m   13: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m   14: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m   15: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m   16: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m   17: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m   18: {'agent0': None},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m   19: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m 2021-03-19 09:37:49,184\tWARNING deprecation.py:33 -- DeprecationWarning: `env_index` has been deprecated. Use `episode.env_id` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m 2021-03-19 09:37:49,184\tINFO sampler.py:835 -- Preprocessed obs: np.ndarray((9,), dtype=float32, min=-1.276, max=1.776, mean=-0.104)\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m 2021-03-19 09:37:49,184\tINFO sampler.py:840 -- Filtered obs: np.ndarray((9,), dtype=float32, min=-1.276, max=1.776, mean=-0.104)\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m 2021-03-19 09:37:49,207\tINFO sampler.py:1249 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-1.276, max=1.776, mean=-0.104),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'env_id': 1,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-0.672, max=1.015, mean=-0.192),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'env_id': 2,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-2.427, max=0.29, mean=-0.419),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'env_id': 3,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-1.26, max=1.364, mean=-0.052),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'env_id': 4,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-0.653, max=1.716, mean=0.241),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'env_id': 5,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-2.25, max=1.028, mean=-0.436),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'env_id': 6,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-2.174, max=2.271, mean=-0.178),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'env_id': 7,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-0.729, max=0.847, mean=0.205),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'env_id': 8,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-1.384, max=3.104, mean=-0.097),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'env_id': 9,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-0.166, max=1.843, mean=0.8),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'env_id': 10,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-1.744, max=0.6, mean=-0.401),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'env_id': 11,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-1.697, max=0.745, mean=-0.188),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'env_id': 12,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-0.878, max=0.23, mean=-0.293),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'env_id': 13,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-1.142, max=0.366, mean=-0.358),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'env_id': 14,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-1.457, max=1.358, mean=0.01),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'env_id': 15,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-1.91, max=1.405, mean=0.043),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'env_id': 16,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-1.907, max=1.191, mean=-0.158),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'env_id': 17,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-1.303, max=1.527, mean=0.085),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'env_id': 18,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-2.183, max=1.399, mean=-0.004),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                         'type': 'PolicyEvalData'},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                       { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'env_id': 19,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'obs': np.ndarray((9,), dtype=float32, min=-0.334, max=1.032, mean=0.339),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'prev_action': np.ndarray((8,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m 2021-03-19 09:37:49,211\tINFO sampler.py:1295 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m { 'default_policy': ( np.ndarray((20, 8), dtype=float32, min=-3.187, max=4.561, mean=0.417),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                       {})}\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m 2021-03-19 09:37:56,531\tINFO sample_batch_builder.py:209 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m { 'agent0': { 'data': { 'actions': np.ndarray((1000, 8), dtype=float32, min=-34.685, max=32.661, mean=-0.043),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                         'agent_index': np.ndarray((1000,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                         'dones': np.ndarray((1000,), dtype=bool, min=0.0, max=1.0, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                         'eps_id': np.ndarray((1000,), dtype=int64, min=936390720.0, max=936390720.0, mean=936390720.0),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                         'infos': np.ndarray((1000,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                         'new_obs': np.ndarray((1000, 9), dtype=float32, min=-55.681, max=1000.0, mean=55.65),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                         'obs': np.ndarray((1000, 9), dtype=float32, min=-55.681, max=999.0, mean=55.539),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                         'prev_actions': np.ndarray((1000, 8), dtype=float32, min=-34.685, max=32.661, mean=-0.041),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                         'prev_rewards': np.ndarray((1000,), dtype=float32, min=-2193.876, max=60.509, mean=-78.084),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                         'rewards': np.ndarray((1000,), dtype=float32, min=-2193.876, max=60.509, mean=-78.069),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                         't': np.ndarray((1000,), dtype=int64, min=0.0, max=999.0, mean=499.5),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m                         'unroll_id': np.ndarray((1000,), dtype=int64, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m 2021-03-19 09:37:56,577\tINFO sample_batch_builder.py:209 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m { 'agent0': { 'data': { 'actions': np.ndarray((1000, 8), dtype=float32, min=-38.505, max=34.105, mean=0.011),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                         'agent_index': np.ndarray((1000,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                         'dones': np.ndarray((1000,), dtype=bool, min=0.0, max=1.0, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                         'eps_id': np.ndarray((1000,), dtype=int64, min=1187488511.0, max=1187488511.0, mean=1187488511.0),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                         'infos': np.ndarray((1000,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                         'new_obs': np.ndarray((1000, 9), dtype=float32, min=-42.907, max=1000.0, mean=55.583),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                         'obs': np.ndarray((1000, 9), dtype=float32, min=-24.553, max=999.0, mean=55.473),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                         'prev_actions': np.ndarray((1000, 8), dtype=float32, min=-38.505, max=34.105, mean=0.01),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                         'prev_rewards': np.ndarray((1000,), dtype=float32, min=-686.467, max=42.593, mean=-58.409),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                         'rewards': np.ndarray((1000,), dtype=float32, min=-686.467, max=42.593, mean=-58.376),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                         't': np.ndarray((1000,), dtype=int64, min=0.0, max=999.0, mean=499.5),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m                         'unroll_id': np.ndarray((1000,), dtype=int64, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m 2021-03-19 09:37:56,557\tINFO sample_batch_builder.py:209 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m { 'agent0': { 'data': { 'actions': np.ndarray((1000, 8), dtype=float32, min=-37.79, max=35.206, mean=-0.064),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                         'agent_index': np.ndarray((1000,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                         'dones': np.ndarray((1000,), dtype=bool, min=0.0, max=1.0, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                         'eps_id': np.ndarray((1000,), dtype=int64, min=783144477.0, max=783144477.0, mean=783144477.0),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                         'infos': np.ndarray((1000,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                         'new_obs': np.ndarray((1000, 9), dtype=float32, min=-25.236, max=1000.0, mean=55.701),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                         'obs': np.ndarray((1000, 9), dtype=float32, min=-25.236, max=999.0, mean=55.591),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                         'prev_actions': np.ndarray((1000, 8), dtype=float32, min=-37.79, max=35.206, mean=-0.065),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                         'prev_rewards': np.ndarray((1000,), dtype=float32, min=-1198.281, max=55.929, mean=-72.211),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                         'rewards': np.ndarray((1000,), dtype=float32, min=-1198.281, max=55.929, mean=-72.214),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                         't': np.ndarray((1000,), dtype=int64, min=0.0, max=999.0, mean=499.5),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m                         'unroll_id': np.ndarray((1000,), dtype=int64, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m 2021-03-19 09:37:56,636\tINFO sample_batch_builder.py:209 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m { 'agent0': { 'data': { 'actions': np.ndarray((1000, 8), dtype=float32, min=-33.25, max=41.507, mean=0.005),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                         'agent_index': np.ndarray((1000,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                         'dones': np.ndarray((1000,), dtype=bool, min=0.0, max=1.0, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                         'eps_id': np.ndarray((1000,), dtype=int64, min=688045019.0, max=688045019.0, mean=688045019.0),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                         'infos': np.ndarray((1000,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                         'new_obs': np.ndarray((1000, 9), dtype=float32, min=-53.861, max=1000.0, mean=55.583),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                         'obs': np.ndarray((1000, 9), dtype=float32, min=-26.811, max=999.0, mean=55.481),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                         'prev_actions': np.ndarray((1000, 8), dtype=float32, min=-33.25, max=41.507, mean=0.007),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                         'prev_rewards': np.ndarray((1000,), dtype=float32, min=-996.244, max=35.797, mean=-72.364),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                         'rewards': np.ndarray((1000,), dtype=float32, min=-996.244, max=35.797, mean=-72.606),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                         't': np.ndarray((1000,), dtype=int64, min=0.0, max=999.0, mean=499.5),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m                         'unroll_id': np.ndarray((1000,), dtype=int64, min=0.0, max=0.0, mean=0.0)},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m 2021-03-19 09:37:56,895\tINFO rollout_worker.py:697 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m { 'data': { 'actions': np.ndarray((20000, 8), dtype=float32, min=-68.83, max=49.196, mean=-0.041),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m             'agent_index': np.ndarray((20000,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m             'dones': np.ndarray((20000,), dtype=bool, min=0.0, max=1.0, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m             'eps_id': np.ndarray((20000,), dtype=int64, min=49052652.0, max=1882163831.0, mean=860839565.7),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m             'infos': np.ndarray((20000,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m             'new_obs': np.ndarray((20000, 9), dtype=float32, min=-51.266, max=1000.0, mean=55.635),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m             'obs': np.ndarray((20000, 9), dtype=float32, min=-48.542, max=999.0, mean=55.523),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m             'prev_actions': np.ndarray((20000, 8), dtype=float32, min=-68.83, max=49.196, mean=-0.041),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m             'prev_rewards': np.ndarray((20000,), dtype=float32, min=-2684.835, max=55.929, mean=-70.006),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m             'rewards': np.ndarray((20000,), dtype=float32, min=-2684.835, max=55.929, mean=-70.034),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m             't': np.ndarray((20000,), dtype=int64, min=0.0, max=999.0, mean=499.5),\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m             'unroll_id': np.ndarray((20000,), dtype=int64, min=0.0, max=19.0, mean=9.5)},\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m 2021-03-19 09:37:56,879\tINFO rollout_worker.py:697 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m { 'data': { 'actions': np.ndarray((20000, 8), dtype=float32, min=-66.575, max=57.119, mean=-0.018),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m             'agent_index': np.ndarray((20000,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m             'dones': np.ndarray((20000,), dtype=bool, min=0.0, max=1.0, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m             'eps_id': np.ndarray((20000,), dtype=int64, min=210499125.0, max=1977538193.0, mean=1226225965.9),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m             'infos': np.ndarray((20000,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m             'new_obs': np.ndarray((20000, 9), dtype=float32, min=-65.19, max=1000.0, mean=55.59),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m             'obs': np.ndarray((20000, 9), dtype=float32, min=-57.18, max=999.0, mean=55.477),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m             'prev_actions': np.ndarray((20000, 8), dtype=float32, min=-66.575, max=57.119, mean=-0.016),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m             'prev_rewards': np.ndarray((20000,), dtype=float32, min=-3110.074, max=65.866, mean=-78.209),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m             'rewards': np.ndarray((20000,), dtype=float32, min=-3110.074, max=65.866, mean=-78.205),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m             't': np.ndarray((20000,), dtype=int64, min=0.0, max=999.0, mean=499.5),\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m             'unroll_id': np.ndarray((20000,), dtype=int64, min=0.0, max=19.0, mean=9.5)},\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m 2021-03-19 09:37:56,930\tINFO rollout_worker.py:697 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m { 'data': { 'actions': np.ndarray((20000, 8), dtype=float32, min=-54.128, max=49.186, mean=0.065),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m             'agent_index': np.ndarray((20000,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m             'dones': np.ndarray((20000,), dtype=bool, min=0.0, max=1.0, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m             'eps_id': np.ndarray((20000,), dtype=int64, min=44066276.0, max=1908118311.0, mean=967810534.45),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m             'infos': np.ndarray((20000,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m             'new_obs': np.ndarray((20000, 9), dtype=float32, min=-74.538, max=1000.0, mean=55.573),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m             'obs': np.ndarray((20000, 9), dtype=float32, min=-53.044, max=999.0, mean=55.458),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m             'prev_actions': np.ndarray((20000, 8), dtype=float32, min=-54.128, max=49.186, mean=0.063),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m             'prev_rewards': np.ndarray((20000,), dtype=float32, min=-2634.357, max=69.303, mean=-61.179),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m             'rewards': np.ndarray((20000,), dtype=float32, min=-2634.357, max=69.303, mean=-61.167),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m             't': np.ndarray((20000,), dtype=int64, min=0.0, max=999.0, mean=499.5),\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m             'unroll_id': np.ndarray((20000,), dtype=int64, min=0.0, max=19.0, mean=9.5)},\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m 2021-03-19 09:37:57,044\tINFO rollout_worker.py:697 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m { 'data': { 'actions': np.ndarray((20000, 8), dtype=float32, min=-56.023, max=65.605, mean=0.013),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m             'agent_index': np.ndarray((20000,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m             'dones': np.ndarray((20000,), dtype=bool, min=0.0, max=1.0, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m             'eps_id': np.ndarray((20000,), dtype=int64, min=153052199.0, max=1999737194.0, mean=1017258847.45),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m             'infos': np.ndarray((20000,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m             'new_obs': np.ndarray((20000, 9), dtype=float32, min=-81.692, max=1000.0, mean=55.6),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m             'obs': np.ndarray((20000, 9), dtype=float32, min=-54.889, max=999.0, mean=55.493),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m             'prev_actions': np.ndarray((20000, 8), dtype=float32, min=-56.023, max=65.605, mean=0.014),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m             'prev_rewards': np.ndarray((20000,), dtype=float32, min=-3509.591, max=70.155, mean=-75.849),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m             'rewards': np.ndarray((20000,), dtype=float32, min=-3509.591, max=70.155, mean=-75.935),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m             't': np.ndarray((20000,), dtype=int64, min=0.0, max=999.0, mean=499.5),\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m             'unroll_id': np.ndarray((20000,), dtype=int64, min=0.0, max=19.0, mean=9.5)},\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m wandb: Waiting for W&B process to finish, PID 3449\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m wandb: Program ended successfully.\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m wandb: Find user logs for this run at: /Users/angelolovatto/Repositories/personal/LQG-SVG/experiments/wandb/offline-run-20210319_093745-2m9h0taw/logs/debug.log\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m wandb: Find internal logs for this run at: /Users/angelolovatto/Repositories/personal/LQG-SVG/experiments/wandb/offline-run-20210319_093745-2m9h0taw/logs/debug-internal.log\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m wandb:          model_epochs 10\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m wandb:             model_nll 683837.5\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m wandb:             iteration 199\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m wandb:   distance_to_optimal 13.97099\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m wandb:    episode_reward_max -74910.76111\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m wandb:    episode_reward_min -148134.70583\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m wandb:   episode_reward_mean -103942.83034\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m wandb:      episode_len_mean 1000.0\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m wandb:    episodes_this_iter 20\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m wandb:              _runtime 3316\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m wandb:            _timestamp 1616160782\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m wandb:                 _step 199\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m wandb:          model_epochs █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▂▁▂▂▂\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m wandb:             model_nll ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▅▅▅▆▆▆▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m wandb:             iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m wandb:   distance_to_optimal ▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m wandb:    episode_reward_max ▆▆▆▆▇█▇██▇██▇█████▇▇▇▇▆▇▆▅▅▅▅▄▄▅▄▄▃▃▄▂▂▁\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m wandb:    episode_reward_min ▇▇▇▇▇▇██▇█▇███▇▇▇▇▇▇▇▆▇▆▇▆▆▆▅▆▄▃▃▄▂▃▁▄▁▃\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m wandb:   episode_reward_mean ▆▆▇▇▇███████████▇▇▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▂▃▂▁\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m wandb:      episode_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m wandb:    episodes_this_iter ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m wandb:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m wandb:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m wandb:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m wandb: You can sync this run to the cloud by running:\n",
      "\u001b[2m\u001b[36m(pid=3411)\u001b[0m wandb: wandb sync /Users/angelolovatto/Repositories/personal/LQG-SVG/experiments/wandb/offline-run-20210319_093745-2m9h0taw\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m wandb: Waiting for W&B process to finish, PID 3448\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m wandb: Program ended successfully.\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m wandb: Find user logs for this run at: /Users/angelolovatto/Repositories/personal/LQG-SVG/experiments/wandb/offline-run-20210319_093745-35kdi4rw/logs/debug.log\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m wandb: Find internal logs for this run at: /Users/angelolovatto/Repositories/personal/LQG-SVG/experiments/wandb/offline-run-20210319_093745-35kdi4rw/logs/debug-internal.log\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m wandb:          model_epochs 6\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m wandb:             model_nll 737272.875\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m wandb:             iteration 199\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m wandb:   distance_to_optimal 13.91369\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m wandb:    episode_reward_max -73735.11095\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m wandb:    episode_reward_min -156189.35356\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m wandb:   episode_reward_mean -102879.45491\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m wandb:      episode_len_mean 1000.0\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m wandb:    episodes_this_iter 20\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m wandb:              _runtime 3335\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m wandb:            _timestamp 1616160802\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m wandb:                 _step 199\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m wandb:          model_epochs █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m wandb:             model_nll ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m wandb:             iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m wandb:   distance_to_optimal ▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m wandb:    episode_reward_max ▅▅▆▇▆▆▇▇▇██▇███▇███▇▇▇▇▆▆▆▇▅▆▅▄▅▄▅▃▄▂▁▁▁\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m wandb:    episode_reward_min ▆▆▇▆▇▇▇██▇█▇█████▇▇█▇█▇▇▇▆▇▆▆▆▆▄▅▅▅▄▃▃▂▁\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m wandb:   episode_reward_mean ▅▆▆▇▇▇██████████████▇▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▃▃▂▁\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m wandb:      episode_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m wandb:    episodes_this_iter ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m wandb:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m wandb:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m wandb:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m wandb: You can sync this run to the cloud by running:\n",
      "\u001b[2m\u001b[36m(pid=3409)\u001b[0m wandb: wandb sync /Users/angelolovatto/Repositories/personal/LQG-SVG/experiments/wandb/offline-run-20210319_093745-35kdi4rw\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m wandb: Waiting for W&B process to finish, PID 3447\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m wandb: Program ended successfully.\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m wandb: Find user logs for this run at: /Users/angelolovatto/Repositories/personal/LQG-SVG/experiments/wandb/offline-run-20210319_093745-ibdq4qi2/logs/debug.log\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m wandb: Find internal logs for this run at: /Users/angelolovatto/Repositories/personal/LQG-SVG/experiments/wandb/offline-run-20210319_093745-ibdq4qi2/logs/debug-internal.log\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m wandb:          model_epochs 6\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m wandb:             model_nll 779603.875\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m wandb:             iteration 199\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m wandb:   distance_to_optimal 13.85005\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m wandb:    episode_reward_max -66582.69838\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m wandb:    episode_reward_min -101995.18495\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m wandb:   episode_reward_mean -84119.04615\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m wandb:      episode_len_mean 1000.0\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m wandb:    episodes_this_iter 20\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m wandb:              _runtime 3347\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m wandb:            _timestamp 1616160813\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m wandb:                 _step 199\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m wandb:          model_epochs █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m wandb:             model_nll ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▅▅▅▆▆▆▆▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m wandb:             iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m wandb:   distance_to_optimal ▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m wandb:    episode_reward_max ▅▇▇▇█▇▇█▇██▇▇▇▇▇▇▇▆▆▆▇▆▇▆▆▅▆▅▄▄▅▂▂▄▄▂▂▂▁\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m wandb:    episode_reward_min ▆▆▇▇▇█████▇▇▇▇▆▇▇▇▇▅▆▆▆▆▆▅▅▅▅▄▅▄▄▃▃▃▂▁▁▂\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m wandb:   episode_reward_mean ▆▆▇▇███████████▇▇▇▇▇▇▇▆▆▆▆▅▅▅▅▄▄▃▃▃▃▂▁▁▁\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m wandb:      episode_len_mean ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m wandb:    episodes_this_iter ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m wandb:              _runtime ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m wandb:            _timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m wandb:                 _step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m wandb: You can sync this run to the cloud by running:\n",
      "\u001b[2m\u001b[36m(pid=3410)\u001b[0m wandb: wandb sync /Users/angelolovatto/Repositories/personal/LQG-SVG/experiments/wandb/offline-run-20210319_093745-ibdq4qi2\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m wandb: Waiting for W&B process to finish, PID 3446\n",
      "\u001b[2m\u001b[36m(pid=3412)\u001b[0m wandb: Program ended successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_exps = [\n",
    "    Experiment.remote(\n",
    "        {\n",
    "            \"iterations\": 200,\n",
    "            \"trajs_per_iter\": 20,\n",
    "            \"policy_lr\": 3e-4,\n",
    "            \"improvement_delta\": 0.0,\n",
    "            \"patience\": 3,\n",
    "            \"svg_samples\": 32,\n",
    "            \"dataset_batch_size\": 32,\n",
    "            \"train_val_split\": (0.8, 0.2),\n",
    "            \"model_lr\": 1e-3,\n",
    "            \"env_config\": dict(n_state=8, n_ctrl=8, horizon=1000, num_envs=20),\n",
    "            \"true_model\": False,\n",
    "        }\n",
    "    )\n",
    "    for _ in range(4)\n",
    "]\n",
    "for exp in big_exps:\n",
    "    exp.setup.remote()\n",
    "    \n",
    "ray.get([e.execute.remote() for e in big_exps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "activated-polish",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
