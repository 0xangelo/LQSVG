{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "wicked-solid",
   "metadata": {},
   "source": [
    "# SVG(inf) for LQG\n",
    "\n",
    "## Introduction\n",
    "In this notebook we implement a simplified version of SVG($\\infty$) for the LQG problem. Our intention is to show how one can learn a parameterized policy in LQG via Stochastic Value Gradients. Note that even if we successfully learn a policy, we do not necessarily know what components of the SVG framework were instrumental in doing so. The focus of the rest of our work will be on analyzing the tenets of this framework based on **gradient estimation quality** and **performance curvature approximation**.\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "virgin-penguin",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import logging\n",
    "import itertools\n",
    "import os.path as osp\n",
    "from datetime import date\n",
    "from numbers import Number\n",
    "from typing import Optional\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "from ray.rllib import RolloutWorker\n",
    "from ray.rllib import SampleBatch\n",
    "from ray.rllib.evaluation.metrics import collect_episodes, summarize_episodes\n",
    "from raylab.policy import TorchPolicy\n",
    "from raylab.policy.modules.actor import DeterministicPolicy\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import TensorDataset\n",
    "from tqdm.auto import trange\n",
    "\n",
    "import lqsvg.torch.named as nt\n",
    "from lqsvg.envs import lqr\n",
    "from lqsvg.envs.lqr.gym import TorchLQGMixin\n",
    "from lqsvg.experiment.data import TrajectoryData\n",
    "from lqsvg.experiment.models import MonteCarloSVG\n",
    "from lqsvg.experiment.models import glorot_init_model\n",
    "from lqsvg.experiment.policy import make_worker\n",
    "from lqsvg.experiment.tqdm_util import collect_with_progress\n",
    "from lqsvg.experiment.utils import group_batch_episodes\n",
    "from lqsvg.experiment.utils import linear_feedback_distance\n",
    "from lqsvg.experiment.utils import suppress_dataloader_warning\n",
    "from lqsvg.policy.time_varying_linear import LQGPolicy\n",
    "\n",
    "# https://github.com/PyTorchLightning/pytorch-lightning/issues/3431\n",
    "logging.getLogger('lightning').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "administrative-storage",
   "metadata": {},
   "source": [
    "---\n",
    "## Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "attempted-louisiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    def __init__(self, run):\n",
    "        self.run = run\n",
    "        \n",
    "        self.worker = None\n",
    "        self.policy = None\n",
    "        self.model = None\n",
    "        self.artifact = None\n",
    "        self.collect_metrics = None\n",
    "        \n",
    "    @property\n",
    "    def dir(self) -> str:\n",
    "        return self.run.dir\n",
    "        \n",
    "    def setup(self):\n",
    "        self.worker = make_worker(env_config)\n",
    "        rllib_policy = self.worker.get_policy()\n",
    "        self.policy = rllib_policy.module.actor\n",
    "        self.model = EnvModel(rllib_policy, run.config)\n",
    "        \n",
    "        self.artifact = self.create_artifact()\n",
    "        self.collect_metrics = CollectMetrics()\n",
    "        \n",
    "    def create_artifact(self) -> wandb.Artifact:\n",
    "        model = self.model.model\n",
    "        return wandb.Artifact(f\"svg_inf-lqg{model.n_state}.{model.n_ctrl}.{model.horizon}\", type=\"model\")\n",
    "\n",
    "    def execute(self):\n",
    "        \"\"\"Main algorithm logic.\"\"\"\n",
    "        hparams = self.run.config\n",
    "        dataset = self.create_dataset()\n",
    "        optimizer = self.create_optimizer(self.policy)\n",
    "\n",
    "        for itr in trange(hparams[\"iterations\"], desc=\"SVG(inf)\", unit=\"iteration\"):\n",
    "            if itr % 10 == 0:\n",
    "                self.add_ckpt_to_artifact(itr)\n",
    "\n",
    "            self.collect_trajs(self.worker, dataset)\n",
    "            self.optimize_model(self.model, dataset)\n",
    "            self.step_policy(self.policy, self.model, optimizer)\n",
    "\n",
    "            # Logging\n",
    "            self.log_iteration(itr)\n",
    "\n",
    "        self.finish()\n",
    "        \n",
    "    def create_dataset(self) -> pl.LightningDataModule:\n",
    "        return DataModule(self.run.config)\n",
    "    \n",
    "    def create_optimizer(self, policy: nn.Module) -> Optimizer:\n",
    "        return torch.optim.Adam(policy.parameters(), lr=self.run.config[\"policy_lr\"])\n",
    "    \n",
    "    def collect_trajs(self, worker: RolloutWorker, dataset: pl.LightningDataModule):\n",
    "        dataset.collect_trajectories(worker, n_trajs=self.run.config[\"trajs_per_iter\"])\n",
    "\n",
    "    def optimize_model(self, model: pl.LightningModule, dataset: pl.LightningDataModule):\n",
    "        hparams = self.run.config\n",
    "\n",
    "        validation_results = ValidationResults()\n",
    "        early_stopping = pl.callbacks.EarlyStopping(\n",
    "            model.early_stop_on,\n",
    "            min_delta=hparams[\"improvement_delta\"],\n",
    "            patience=hparams[\"patience\"],\n",
    "            mode=\"min\",\n",
    "            strict=True,\n",
    "        )\n",
    "        trainer = pl.Trainer(\n",
    "            default_root_dir=self.dir,\n",
    "            callbacks=[early_stopping, validation_results], \n",
    "            max_epochs=1000, \n",
    "            progress_bar_refresh_rate=0,  # don't show progress bar for model training\n",
    "            weights_summary=None,  # don't print summary before training\n",
    "            checkpoint_callback=False,  # don't save last model\n",
    "        )\n",
    "        with suppress_dataloader_warning():\n",
    "            trainer.fit(model, datamodule=dataset)\n",
    "\n",
    "        # Logging\n",
    "        results = {\n",
    "            \"model_epochs\": trainer.current_epoch + 1, \n",
    "            \"model_nll\": validation_results.last_validation_loss().item(),\n",
    "        }\n",
    "        self.run.log(results, commit=False)\n",
    "\n",
    "    def step_policy(self, policy: DeterministicPolicy, model: pl.LightningModule, optimizer: Optimizer):\n",
    "        svg = MonteCarloSVG(policy, model.model)\n",
    "        optimizer.zero_grad()\n",
    "        loss = -svg.value(self.run.config[\"svg_samples\"])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    def log_iteration(self, itr: int):\n",
    "        pistar, _, _ = self.worker.env.solution\n",
    "        logs = {\n",
    "            \"iteration\": itr, \n",
    "            \"distance_to_optimal\": linear_feedback_distance(self.policy.standard_form(), pistar), \n",
    "            **self.collect_metrics(self.worker)\n",
    "        }\n",
    "        self.run.log(logs)\n",
    "        \n",
    "    def finish(self):\n",
    "        self.add_ckpt_to_artifact(self.run.config[\"iterations\"])            \n",
    "        self.run.log_artifact(self.artifact)        \n",
    "\n",
    "    def add_ckpt_to_artifact(self, itr: int):\n",
    "        policy: TorchPolicy = self.worker.get_policy()\n",
    "        path = osp.join(self.run.dir, f\"module-iter={itr}.pt\")\n",
    "        torch.save(policy.module.state_dict(), path)\n",
    "        self.artifact.add_file(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-creativity",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "double-broadcast",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollectMetrics:\n",
    "    # Copied from https://github.com/ray-project/ray/blob/c409b5b63a6928e423428b700e528e35d791e8ea/rllib/execution/metric_ops.py#L47\n",
    "    def __init__(self):\n",
    "        self.episode_history = []\n",
    "        self.to_be_collected = []\n",
    "\n",
    "    def __call__(self, worker: RolloutWorker, min_history: int = 100, timeout_seconds: int = 180) -> dict:\n",
    "        # Collect worker metrics.\n",
    "        episodes, self.to_be_collected = collect_episodes(\n",
    "            worker, to_be_collected=self.to_be_collected, timeout_seconds=timeout_seconds\n",
    "        )\n",
    "        orig_episodes = list(episodes)\n",
    "        missing = min_history - len(episodes)\n",
    "        if missing > 0:\n",
    "            episodes.extend(self.episode_history[-missing:])\n",
    "            assert len(episodes) <= min_history\n",
    "        self.episode_history.extend(orig_episodes)\n",
    "        self.episode_history = self.episode_history[-min_history:]\n",
    "        return {k: v for k, v in summarize_episodes(episodes, orig_episodes).items() if isinstance(v, Number)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "mobile-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationResults(pl.callbacks.Callback):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.saved_outputs = None\n",
    "\n",
    "    def on_validation_epoch_start(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n",
    "        self.saved_outputs: list[Tensor] = []\n",
    "            \n",
    "    def on_validation_batch_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule, outputs: Tensor, *args, **kwargs):\n",
    "        self.saved_outputs += [outputs]\n",
    "        \n",
    "    def last_validation_loss(self) -> Tensor:\n",
    "        return torch.stack(self.saved_outputs, dim=0).mean(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-wholesale",
   "metadata": {},
   "source": [
    "### Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "outer-lodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, hparams: dict):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_size: float = hparams[\"dataset_batch_size\"]\n",
    "        self.train_val_split: tuple[float, float] = hparams[\"train_val_split\"]\n",
    "\n",
    "        self.full_dataset = None\n",
    "        self.train_dataset, self.val_dataset = None, None\n",
    "        self._itr_datasets: list[TensorDataset] = []\n",
    "        \n",
    "    def collect_trajectories(self, rollout_worker: RolloutWorker, n_trajs: int):\n",
    "        worker = rollout_worker\n",
    "        self._check_rollout_worker(worker)\n",
    "        \n",
    "        sample_batch = collect_with_progress(worker, n_trajs, prog=False)\n",
    "        sample_batch = group_batch_episodes(sample_batch)\n",
    "        trajs = sample_batch.split_by_episode()\n",
    "        self._check_collected_trajs(trajs, worker, n_trajs)\n",
    "        \n",
    "        traj_dataset = TrajectoryData.trajectory_dataset(trajs)\n",
    "        self._itr_datasets += [traj_dataset]\n",
    "        self.full_dataset = ConcatDataset(self._itr_datasets)\n",
    "        \n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        del stage\n",
    "        train_frac, _ = self.train_val_split\n",
    "        train_trajs = int(train_frac * len(self.full_dataset))\n",
    "        val_trajs = len(self.full_dataset) - train_trajs\n",
    "\n",
    "        self.train_dataset, self.val_dataset = random_split(self.full_dataset, (train_trajs, val_trajs))\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        # pylint:disable=arguments-differ\n",
    "        return DataLoader(self.train_dataset, shuffle=True, batch_size=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        # pylint:disable=arguments-differ\n",
    "        return DataLoader(self.val_dataset, shuffle=False, batch_size=self.batch_size)\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_rollout_worker(worker: RolloutWorker):\n",
    "        assert worker.rollout_fragment_length == worker.env.horizon * worker.num_envs\n",
    "        assert worker.batch_mode == \"truncate_episodes\"\n",
    "        \n",
    "    @staticmethod\n",
    "    def _check_collected_trajs(trajs: list[SampleBatch], worker: RolloutWorker, n_trajs: int):\n",
    "        traj_counts = [t.count for t in trajs]\n",
    "        assert all(c == worker.env.horizon for c in traj_counts), traj_counts\n",
    "        total_ts = sum(t.count for t in trajs)\n",
    "        assert total_ts == n_trajs * worker.env.horizon, total_ts        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "satellite-cancellation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvModel(pl.LightningModule):\n",
    "    early_stop_on: str = \"val/loss\"\n",
    "\n",
    "    def __init__(self, policy: LQGPolicy, hparams: dict):\n",
    "        super().__init__()\n",
    "        self.model = policy.module.model\n",
    "\n",
    "        self.hparams.learning_rate = hparams[\"model_lr\"]\n",
    "        glorot_init_model(self.model)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        params = nn.ParameterList(\n",
    "            itertools.chain(self.model.trans.parameters(), self.model.init.parameters())\n",
    "        )\n",
    "        optim = torch.optim.Adam(params, lr=self.hparams.learning_rate)\n",
    "        return optim\n",
    "\n",
    "    def forward(self, obs: Tensor, act: Tensor, new_obs: Tensor) -> Tensor:\n",
    "        \"\"\"Batched trajectory log prob.\"\"\"\n",
    "        # pylint:disable=arguments-differ\n",
    "        return self.model.log_prob(obs, act, new_obs)\n",
    "\n",
    "    def _compute_loss_on_batch(\n",
    "        self, batch: tuple[Tensor, Tensor, Tensor], batch_idx: int\n",
    "    ) -> Tensor:\n",
    "        del batch_idx\n",
    "        obs, act, new_obs = (x.refine_names(\"B\", \"H\", \"R\") for x in batch)\n",
    "        return -self(obs, act, new_obs).mean()\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: tuple[Tensor, Tensor, Tensor], batch_idx: int\n",
    "    ) -> Tensor:\n",
    "        # pylint:disable=arguments-differ\n",
    "        loss = self._compute_loss_on_batch(batch, batch_idx)\n",
    "        self.log(\"train/loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(\n",
    "        self, batch: tuple[Tensor, Tensor, Tensor], batch_idx: int\n",
    "    ) -> Tensor:\n",
    "        # pylint:disable=arguments-differ\n",
    "        loss = self._compute_loss_on_batch(batch, batch_idx)\n",
    "        self.log(\"val/loss\", loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "likely-rwanda",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "norwegian-biography",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CalVer: 3.17.0\n"
     ]
    }
   ],
   "source": [
    "def calver() -> str:\n",
    "    today = date.today()\n",
    "    return f\"{today.month}.{today.day}.0\"\n",
    "\n",
    "print(\"CalVer:\", calver())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "union-music",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mangelovtt\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.22<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">SVG(inf)</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/angelovtt/LQG-SVG\" target=\"_blank\">https://wandb.ai/angelovtt/LQG-SVG</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/angelovtt/LQG-SVG/runs/10x8rgrc\" target=\"_blank\">https://wandb.ai/angelovtt/LQG-SVG/runs/10x8rgrc</a><br/>\n",
       "                Run data is saved locally in <code>/Users/angelolovatto/Repositories/personal/LQG-SVG/experiments/wandb/run-20210317_130801-10x8rgrc</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d06f649066e470db7c77d78f6fa5e23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SVG(inf):   0%|          | 0/200 [00:00<?, ?iteration/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-17 13:08:04,391\tWARNING deprecation.py:33 -- DeprecationWarning: `env_index` has been deprecated. Use `episode.env_id` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 15233<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 44.60MB of 44.60MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/Users/angelolovatto/Repositories/personal/LQG-SVG/experiments/wandb/run-20210317_130801-10x8rgrc/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/Users/angelolovatto/Repositories/personal/LQG-SVG/experiments/wandb/run-20210317_130801-10x8rgrc/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>model_epochs</td><td>10</td></tr><tr><td>model_nll</td><td>639261.75</td></tr><tr><td>iteration</td><td>199</td></tr><tr><td>distance_to_optimal</td><td>13.90417</td></tr><tr><td>episode_reward_max</td><td>-61931.17195</td></tr><tr><td>episode_reward_min</td><td>-101351.81497</td></tr><tr><td>episode_reward_mean</td><td>-77556.79786</td></tr><tr><td>episode_len_mean</td><td>1000.0</td></tr><tr><td>episodes_this_iter</td><td>20</td></tr><tr><td>_runtime</td><td>1649</td></tr><tr><td>_timestamp</td><td>1615998930</td></tr><tr><td>_step</td><td>199</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>model_epochs</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▂▁▂▂▂▂▂</td></tr><tr><td>model_nll</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▅▅▅▆▆▆▆▇▇▇▇██</td></tr><tr><td>iteration</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>distance_to_optimal</td><td>▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██</td></tr><tr><td>episode_reward_max</td><td>▃▄▅▅▆▆▇▇▇▇▇█▇███▇▇▇██▇▇▇▇▆▆▅▄▄▅▄▄▄▄▂▃▁▁▁</td></tr><tr><td>episode_reward_min</td><td>▆▅▅▇▇▆█▇▇██▆▆▇▇▆▅▇▇▆▆▅▆▆▆▆▄▅▅▄▃▄▃▃▃▂▂▃▂▁</td></tr><tr><td>episode_reward_mean</td><td>▅▅▆▆▇▇▇▇▇███████▇▇▇▇▇▆▆▇▆▆▆▅▅▅▅▅▄▄▃▃▂▂▁▁</td></tr><tr><td>episode_len_mean</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>episodes_this_iter</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 0 media file(s), 23 artifact file(s) and 422 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">SVG(inf)</strong>: <a href=\"https://wandb.ai/angelovtt/LQG-SVG/runs/10x8rgrc\" target=\"_blank\">https://wandb.ai/angelovtt/LQG-SVG/runs/10x8rgrc</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env_config = dict(n_state=8, n_ctrl=8, horizon=1000, num_envs=20)\n",
    "hparams = {\n",
    "    \"iterations\": 200,\n",
    "    \"trajs_per_iter\": 20,\n",
    "    \"policy_lr\": 3e-4,\n",
    "    \"improvement_delta\": 0.0,\n",
    "    \"patience\": 3,\n",
    "    \"svg_samples\": 32,\n",
    "    \"dataset_batch_size\": 32,\n",
    "    \"train_val_split\": (0.8, 0.2),\n",
    "    \"model_lr\": 1e-3,\n",
    "    \"env_config\": env_config,\n",
    "}\n",
    "\n",
    "run = wandb.init(\n",
    "    name=\"SVG(inf)\",\n",
    "    project=\"LQG-SVG\",\n",
    "    entity=\"angelovtt\",\n",
    "    tags=[calver()],\n",
    "    mode=\"online\",\n",
    "    save_code=True,\n",
    "    job_type=\"train\",\n",
    ")\n",
    "run.config.update(hparams)\n",
    "\n",
    "with run, nt.suppress_named_tensor_warning():\n",
    "    experiment = Experiment(run)\n",
    "    experiment.setup()\n",
    "    experiment.execute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
