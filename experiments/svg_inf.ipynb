{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "wanted-laptop",
   "metadata": {},
   "source": [
    "# SVG(inf) for LQG\n",
    "\n",
    "## Introduction\n",
    "In this notebook we implement a simplified version of SVG($\\infty$) for the LQG problem. Our intention is to show how one can learn a parameterized policy in LQG via Stochastic Value Gradients. Note that even if we successfully learn a policy, we do not necessarily know what components of the SVG framework were instrumental in doing so. The focus of the rest of our work will be on analyzing the tenets of this framework based on **gradient estimation quality** and **performance curvature approximation**.\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "leading-darkness",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import logging\n",
    "import itertools\n",
    "import os.path as osp\n",
    "from datetime import date\n",
    "from numbers import Number\n",
    "from typing import Optional\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "from ray.rllib import RolloutWorker\n",
    "from ray.rllib import SampleBatch\n",
    "from ray.rllib.evaluation.metrics import collect_episodes, summarize_episodes\n",
    "from raylab.policy import TorchPolicy\n",
    "from raylab.policy.modules.actor import DeterministicPolicy\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import TensorDataset\n",
    "from tqdm.auto import trange\n",
    "\n",
    "import lqsvg.torch.named as nt\n",
    "from lqsvg.envs import lqr\n",
    "from lqsvg.envs.lqr.gym import TorchLQGMixin\n",
    "from lqsvg.policy.time_varying_linear import LQGPolicy\n",
    "\n",
    "from data import TrajectoryData  # pylint:disable=wrong-import-order\n",
    "from models import MonteCarloSVG  # pylint:disable=wrong-import-order\n",
    "from models import glorot_init_model  # pylint:disable=wrong-import-order\n",
    "from policy import make_worker  # pylint:disable=wrong-import-order\n",
    "from tqdm_util import collect_with_progress  # pylint:disable=wrong-import-order\n",
    "from utils import group_batch_episodes  # pylint:disable=wrong-import-order\n",
    "from utils import linear_feedback_distance  # pylint:disable=wrong-import-order\n",
    "from utils import suppress_dataloader_warning  # pylint:disable=wrong-import-order\n",
    "\n",
    "# https://github.com/PyTorchLightning/pytorch-lightning/issues/3431\n",
    "logging.getLogger('lightning').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-confidentiality",
   "metadata": {},
   "source": [
    "---\n",
    "## Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "norman-exhaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(rollout_worker: RolloutWorker, hparams: dict) -> pl.LightningDataModule:\n",
    "    del rollout_worker\n",
    "    return DataModule(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "completed-geography",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(policy: DeterministicPolicy, hparams: dict) -> Optimizer:\n",
    "    return torch.optim.Adam(policy.parameters(), lr=hparams[\"policy_lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "rubber-shift",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_trajs(rollout_worker: RolloutWorker, dataset: pl.LightningDataModule, hparams: dict):\n",
    "    dataset.collect_trajectories(rollout_worker, n_trajs=hparams[\"trajs_per_iter\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "operational-defeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationResults(pl.callbacks.Callback):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.saved_outputs = None\n",
    "\n",
    "    def on_validation_epoch_start(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n",
    "        self.saved_outputs: list[Tensor] = []\n",
    "            \n",
    "    def on_validation_batch_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule, outputs: Tensor, *args, **kwargs):\n",
    "        self.saved_outputs += [outputs]\n",
    "        \n",
    "    def last_validation_loss(self) -> Tensor:\n",
    "        return torch.stack(self.saved_outputs, dim=0).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "governing-robert",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(model: pl.LightningModule, dataset: pl.LightningDataModule, run):\n",
    "    hparams = run.config\n",
    "\n",
    "    validation_results = ValidationResults()\n",
    "    early_stopping = pl.callbacks.EarlyStopping(\n",
    "        model.early_stop_on,\n",
    "        min_delta=hparams[\"improvement_delta\"],\n",
    "        patience=hparams[\"patience\"],\n",
    "        mode=\"min\",\n",
    "        strict=True,\n",
    "    )\n",
    "    trainer = pl.Trainer(\n",
    "        default_root_dir=run.dir,\n",
    "        callbacks=[early_stopping, validation_results], \n",
    "        max_epochs=1000, \n",
    "        progress_bar_refresh_rate=0,  # don't show progress bar for model training\n",
    "        weights_summary=None,  # don't print summary before training\n",
    "        checkpoint_callback=False,  # don't save last model\n",
    "    )\n",
    "    with suppress_dataloader_warning():\n",
    "        trainer.fit(model, datamodule=dataset)\n",
    "\n",
    "    # Logging\n",
    "    results = {\n",
    "        \"model_epochs\": trainer.current_epoch + 1, \n",
    "        \"model_nll\": validation_results.last_validation_loss().item(),\n",
    "    }\n",
    "    run.log(results, commit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "upper-garden",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_policy(policy: DeterministicPolicy, model: pl.LightningModule, optimizer: Optimizer, hparams: dict):\n",
    "    svg = MonteCarloSVG(policy, model.model)\n",
    "    optimizer.zero_grad()\n",
    "    loss = -svg.value(hparams[\"svg_samples\"])\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "capital-sharp",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svg_inf(policy: DeterministicPolicy, model: pl.LightningModule, rollout_worker: RolloutWorker, run):    \n",
    "    artifact = wandb.Artifact(f\"svg_inf-lqg{model.model.n_state}.{model.model.n_ctrl}.{model.model.horizon}\", type=\"model\")\n",
    "    collect_metrics = CollectMetrics()\n",
    "\n",
    "    ### Main algorithm logic ###\n",
    "    dataset = create_dataset(rollout_worker, run.config)\n",
    "    optimizer = create_optimizer(policy, run.config)\n",
    "    for itr in trange(hparams[\"iterations\"], desc=\"SVG(inf)\", unit=\"iteration\"):\n",
    "        if itr % 10 == 0:\n",
    "            add_ckpt_to_artifact(itr, artifact, rollout_worker.get_policy(), run)\n",
    "\n",
    "        collect_trajs(rollout_worker, dataset, run.config)\n",
    "        optimize_model(model, dataset, run)\n",
    "        step_policy(policy, model, optimizer, run.config)\n",
    "    ############################\n",
    "        \n",
    "        # Logging\n",
    "        pistar, _, _ = rollout_worker.env.solution\n",
    "        logs = {\n",
    "            \"iteration\": itr, \n",
    "            \"distance_to_optimal\": linear_feedback_distance(policy.standard_form(), pistar), \n",
    "            **collect_metrics(rollout_worker)\n",
    "        }\n",
    "        run.log(logs)\n",
    "        \n",
    "    add_ckpt_to_artifact(hparams[\"iterations\"], artifact, rollout_worker.get_policy(), run)            \n",
    "    run.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ambient-circle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ckpt_to_artifact(itr: int, artifact: wandb.Artifact, policy: TorchPolicy, run):\n",
    "    path = osp.join(run.dir, f\"module-iter={itr}.pt\")\n",
    "    torch.save(policy.module.state_dict(), path)\n",
    "    artifact.add_file(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-genesis",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "champion-sucking",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollectMetrics:\n",
    "    # Copied from https://github.com/ray-project/ray/blob/c409b5b63a6928e423428b700e528e35d791e8ea/rllib/execution/metric_ops.py#L47\n",
    "    def __init__(self):\n",
    "        self.episode_history = []\n",
    "        self.to_be_collected = []\n",
    "\n",
    "    def __call__(self, worker: RolloutWorker, min_history: int = 100, timeout_seconds: int = 180) -> dict:\n",
    "        # Collect worker metrics.\n",
    "        episodes, self.to_be_collected = collect_episodes(\n",
    "            worker, to_be_collected=self.to_be_collected, timeout_seconds=timeout_seconds\n",
    "        )\n",
    "        orig_episodes = list(episodes)\n",
    "        missing = min_history - len(episodes)\n",
    "        if missing > 0:\n",
    "            episodes.extend(self.episode_history[-missing:])\n",
    "            assert len(episodes) <= min_history\n",
    "        self.episode_history.extend(orig_episodes)\n",
    "        self.episode_history = self.episode_history[-min_history:]\n",
    "        return {k: v for k, v in summarize_episodes(episodes, orig_episodes).items() if isinstance(v, Number)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civil-roots",
   "metadata": {},
   "source": [
    "### Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "relative-exhibit",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, hparams: dict):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_size: float = hparams[\"dataset_batch_size\"]\n",
    "        self.train_val_split: tuple[float, float] = hparams[\"train_val_split\"]\n",
    "\n",
    "        self.full_dataset = None\n",
    "        self.train_dataset, self.val_dataset = None, None\n",
    "        self._itr_datasets: list[TensorDataset] = []\n",
    "        \n",
    "    def collect_trajectories(self, rollout_worker: RolloutWorker, n_trajs: int):\n",
    "        worker = rollout_worker\n",
    "        self._check_rollout_worker(worker)\n",
    "        \n",
    "        sample_batch = collect_with_progress(worker, n_trajs, prog=False)\n",
    "        sample_batch = group_batch_episodes(sample_batch)\n",
    "        trajs = sample_batch.split_by_episode()\n",
    "        self._check_collected_trajs(trajs, worker, n_trajs)\n",
    "        \n",
    "        traj_dataset = TrajectoryData.trajectory_dataset(trajs)\n",
    "        self._itr_datasets += [traj_dataset]\n",
    "        self.full_dataset = ConcatDataset(self._itr_datasets)\n",
    "        \n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        del stage\n",
    "        train_frac, _ = self.train_val_split\n",
    "        train_trajs = int(train_frac * len(self.full_dataset))\n",
    "        val_trajs = len(self.full_dataset) - train_trajs\n",
    "\n",
    "        self.train_dataset, self.val_dataset = random_split(self.full_dataset, (train_trajs, val_trajs))\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        # pylint:disable=arguments-differ\n",
    "        return DataLoader(self.train_dataset, shuffle=True, batch_size=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        # pylint:disable=arguments-differ\n",
    "        return DataLoader(self.val_dataset, shuffle=False, batch_size=self.batch_size)\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_rollout_worker(worker: RolloutWorker):\n",
    "        assert worker.rollout_fragment_length == worker.env.horizon * worker.num_envs\n",
    "        assert worker.batch_mode == \"truncate_episodes\"\n",
    "        \n",
    "    @staticmethod\n",
    "    def _check_collected_trajs(trajs: list[SampleBatch], worker: RolloutWorker, n_trajs: int):\n",
    "        traj_counts = [t.count for t in trajs]\n",
    "        assert all(c == worker.env.horizon for c in traj_counts), traj_counts\n",
    "        total_ts = sum(t.count for t in trajs)\n",
    "        assert total_ts == n_trajs * worker.env.horizon, total_ts        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "welsh-court",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvModel(pl.LightningModule):\n",
    "    early_stop_on: str = \"val/loss\"\n",
    "\n",
    "    def __init__(self, policy: LQGPolicy, hparams: dict):\n",
    "        super().__init__()\n",
    "        self.model = policy.module.model\n",
    "\n",
    "        self.hparams.learning_rate = hparams[\"model_lr\"]\n",
    "        glorot_init_model(self.model)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        params = nn.ParameterList(\n",
    "            itertools.chain(self.model.trans.parameters(), self.model.init.parameters())\n",
    "        )\n",
    "        optim = torch.optim.Adam(params, lr=self.hparams.learning_rate)\n",
    "        return optim\n",
    "\n",
    "    def forward(self, obs: Tensor, act: Tensor, new_obs: Tensor) -> Tensor:\n",
    "        \"\"\"Batched trajectory log prob.\"\"\"\n",
    "        # pylint:disable=arguments-differ\n",
    "        return self.model.log_prob(obs, act, new_obs)\n",
    "\n",
    "    def _compute_loss_on_batch(\n",
    "        self, batch: tuple[Tensor, Tensor, Tensor], batch_idx: int\n",
    "    ) -> Tensor:\n",
    "        del batch_idx\n",
    "        obs, act, new_obs = (x.refine_names(\"B\", \"H\", \"R\") for x in batch)\n",
    "        return -self(obs, act, new_obs).mean()\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: tuple[Tensor, Tensor, Tensor], batch_idx: int\n",
    "    ) -> Tensor:\n",
    "        # pylint:disable=arguments-differ\n",
    "        loss = self._compute_loss_on_batch(batch, batch_idx)\n",
    "        self.log(\"train/loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(\n",
    "        self, batch: tuple[Tensor, Tensor, Tensor], batch_idx: int\n",
    "    ) -> Tensor:\n",
    "        # pylint:disable=arguments-differ\n",
    "        loss = self._compute_loss_on_batch(batch, batch_idx)\n",
    "        self.log(\"val/loss\", loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-whale",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ambient-convertible",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CalVer: 3.17.0\n"
     ]
    }
   ],
   "source": [
    "def calver() -> str:\n",
    "    today = date.today()\n",
    "    return f\"{today.month}.{today.day}.0\"\n",
    "\n",
    "print(\"CalVer:\", calver())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "amended-minister",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mangelovtt\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.22<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">SVG(inf)</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/angelovtt/LQG-SVG\" target=\"_blank\">https://wandb.ai/angelovtt/LQG-SVG</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/angelovtt/LQG-SVG/runs/11tbe2go\" target=\"_blank\">https://wandb.ai/angelovtt/LQG-SVG/runs/11tbe2go</a><br/>\n",
       "                Run data is saved locally in <code>/Users/angelolovatto/Repositories/personal/LQG-SVG/experiments/wandb/run-20210317_105317-11tbe2go</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cdaa87ab25949268cde81e7ded2b5cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SVG(inf):   0%|          | 0/200 [00:00<?, ?iteration/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-17 10:53:19,507\tWARNING deprecation.py:33 -- DeprecationWarning: `env_index` has been deprecated. Use `episode.env_id` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 10257<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.76MB of 0.76MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/Users/angelolovatto/Repositories/personal/LQG-SVG/experiments/wandb/run-20210317_105317-11tbe2go/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/Users/angelolovatto/Repositories/personal/LQG-SVG/experiments/wandb/run-20210317_105317-11tbe2go/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>model_epochs</td><td>4</td></tr><tr><td>model_nll</td><td>386.00287</td></tr><tr><td>iteration</td><td>199</td></tr><tr><td>distance_to_optimal</td><td>3.4168</td></tr><tr><td>episode_reward_max</td><td>-259.51477</td></tr><tr><td>episode_reward_min</td><td>-956.11107</td></tr><tr><td>episode_reward_mean</td><td>-552.59653</td></tr><tr><td>episode_len_mean</td><td>100.0</td></tr><tr><td>episodes_this_iter</td><td>20</td></tr><tr><td>_runtime</td><td>171</td></tr><tr><td>_timestamp</td><td>1615989368</td></tr><tr><td>_step</td><td>199</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>model_epochs</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>model_nll</td><td>▁▁▂▂▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇██</td></tr><tr><td>iteration</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>distance_to_optimal</td><td>██▇▇▇▇▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>episode_reward_max</td><td>▁▃▂▂▅▆▆▆▂▂▅▃▃▄▂▇▃▄▆▂▃▅▅▄▄▇▄█▅▅▄▅▅▇▇▇█▆▅▇</td></tr><tr><td>episode_reward_min</td><td>▅▆▁▁▇▇▆▆▆▅▆▆▆▇▆▇▆▇▅▇▅▇▆▇▇▇██▇▇▇▅▆▇▆▆▇▆▇█</td></tr><tr><td>episode_reward_mean</td><td>▂▁▁▃▄▄▃▃▃▃▃▄▂▄▄▆▄▆▆▆▅▆▆▄▆█▇█▇▇▆▇▇▇▆▇█▇██</td></tr><tr><td>episode_len_mean</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>episodes_this_iter</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 0 media file(s), 23 artifact file(s) and 423 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">SVG(inf)</strong>: <a href=\"https://wandb.ai/angelovtt/LQG-SVG/runs/11tbe2go\" target=\"_blank\">https://wandb.ai/angelovtt/LQG-SVG/runs/11tbe2go</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    name=\"SVG(inf)\",\n",
    "    project=\"LQG-SVG\",\n",
    "    entity=\"angelovtt\",\n",
    "    tags=[calver()],\n",
    "    mode=\"online\",\n",
    "    save_code=True,\n",
    "    job_type=\"train\",\n",
    ")\n",
    "\n",
    "with run, nt.suppress_named_tensor_warning():\n",
    "    env_config = dict(n_state=2, n_ctrl=2, horizon=100, num_envs=20)\n",
    "    hparams = {\n",
    "        \"iterations\": 200,\n",
    "        \"trajs_per_iter\": 20,\n",
    "        \"policy_lr\": 3e-4,\n",
    "        \"improvement_delta\": 0.0,\n",
    "        \"patience\": 3,\n",
    "        \"svg_samples\": 32,\n",
    "        \"dataset_batch_size\": 32,\n",
    "        \"train_val_split\": (0.8, 0.2),\n",
    "        \"model_lr\": 1e-3,\n",
    "        \"env_config\": env_config,\n",
    "    }\n",
    "    \n",
    "    run.config.update(hparams)\n",
    "    worker = make_worker(env_config)\n",
    "    rllib_policy = worker.get_policy()\n",
    "    policy = rllib_policy.module.actor\n",
    "    model = EnvModel(rllib_policy, run.config)\n",
    "    \n",
    "    svg_inf(policy, model, worker, run)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
